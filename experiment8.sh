Apollo.io

Perfect â€” hereâ€™s a **clean, â€œsexyâ€ (i.e. hackathon-winning), end-to-end technical solution** with **models, pipeline, and where an OpenAI LLM API fits**, written fully in **professional English**. --- # ðŸš€ Solution Architecture ## **APIx: Aadhaar Predictive Intelligence & Early-Warning Syst

pasted


List of Datasets 1. Aadhaar Enrolment dataset This dataset provides aggregated information on Aadhaar enrolments across various demographic and geographic levels. It includes variables such as the date of enrollment, state, district, PIN code, and age-wise categories (0â€“5 years, 5â€“17 years, and 18 y

pasted

GIVE ME A .ipynb notebook wherein i will upload the 3 datasets, process them and generate a sexy NO EMOJI pipeline which solves this problem

10:21 PM
I'll create a comprehensive Jupyter notebook for your Aadhaar Predictive Intelligence hackathon solution. Let me first check the relevant skills for creating high-quality outputs.


Check available skills for creating the notebook

Now let me create a comprehensive, professional Jupyter notebook for your hackathon solution:


Creating comprehensive Jupyter notebook for Aadhaar Predictive Intelligence hackathon
APIx_Aadhaar_Predictive_Intelligence.ipynb
/home/claude/APIx_Aadhaar_Predictive_Intelligence.ipynb

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIx: Aadhaar Predictive Intelligence & Early-Warning System\n",
    "\n",
    "**A Data-Driven Early Warning System for Aadhaar Infrastructure Stress Prediction**\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This notebook implements a comprehensive analytical pipeline that transforms raw Aadhaar enrolment and update data into actionable intelligence. The system predicts where Aadhaar infrastructure will face stress before citizens experience delays or failures.\n",
    "\n",
    "**Core Components:**\n",
    "1. Data Ingestion and Cleaning\n",
    "2. Feature Engineering (Stress Ratios, Age Transition Indices, Volatility Scores)\n",
    "3. Exploratory Data Analysis (Univariate, Bivariate, Trivariate)\n",
    "4. Predictive Modeling (Classification, Time-Series Forecasting, Anomaly Detection)\n",
    "5. LLM-Powered Explainability Layer\n",
    "6. Decision Framework and Recommendations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Time Series\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# LLM Integration\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Display Settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Plot Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Environment configured successfully.\")\n",
    "print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Ingestion Layer\n",
    "\n",
    "Load the three core datasets:\n",
    "- Aadhaar Enrolment Dataset\n",
    "- Aadhaar Demographic Update Dataset  \n",
    "- Aadhaar Biometric Update Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING CONFIGURATION\n",
    "# =============================================================================\n",
    "# Update these paths to match your dataset locations\n",
    "\n",
    "ENROLMENT_PATH = 'aadhaar_enrolment.csv'          # Update with your path\n",
    "DEMOGRAPHIC_PATH = 'aadhaar_demographic_update.csv'  # Update with your path\n",
    "BIOMETRIC_PATH = 'aadhaar_biometric_update.csv'   # Update with your path\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD DATASETS\n",
    "# =============================================================================\n",
    "\n",
    "def load_dataset(filepath, dataset_name):\n",
    "    \"\"\"Load dataset with error handling and initial diagnostics.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"[SUCCESS] {dataset_name}\")\n",
    "        print(f\"   Shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "        print(f\"   Columns: {list(df.columns)}\")\n",
    "        print()\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] {dataset_name} not found at: {filepath}\")\n",
    "        print(\"   Please update the path variable above.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load {dataset_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATA INGESTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_enrol = load_dataset(ENROLMENT_PATH, \"Enrolment Dataset\")\n",
    "df_demo = load_dataset(DEMOGRAPHIC_PATH, \"Demographic Update Dataset\")\n",
    "df_bio = load_dataset(BIOMETRIC_PATH, \"Biometric Update Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INITIAL DATA INSPECTION\n",
    "# =============================================================================\n",
    "\n",
    "def inspect_dataset(df, name):\n",
    "    \"\"\"Comprehensive initial inspection of dataset.\"\"\"\n",
    "    if df is None:\n",
    "        print(f\"Dataset {name} not loaded.\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"INSPECTION: {name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n--- First 5 Rows ---\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\n--- Data Types ---\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\n--- Missing Values ---\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df) * 100).round(2)\n",
    "    missing_df = pd.DataFrame({'Missing': missing, 'Percentage': missing_pct})\n",
    "    print(missing_df[missing_df['Missing'] > 0] if missing.sum() > 0 else \"No missing values detected.\")\n",
    "    \n",
    "    print(\"\\n--- Statistical Summary ---\")\n",
    "    display(df.describe())\n",
    "    print()\n",
    "\n",
    "inspect_dataset(df_enrol, \"Enrolment Dataset\")\n",
    "inspect_dataset(df_demo, \"Demographic Update Dataset\")\n",
    "inspect_dataset(df_bio, \"Biometric Update Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA CLEANING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "class DataCleaner:\n",
    "    \"\"\"Comprehensive data cleaning pipeline for Aadhaar datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cleaning_log = []\n",
    "    \n",
    "    def log(self, message):\n",
    "        \"\"\"Log cleaning operations.\"\"\"\n",
    "        timestamp = datetime.now().strftime('%H:%M:%S')\n",
    "        self.cleaning_log.append(f\"[{timestamp}] {message}\")\n",
    "        print(f\"   {message}\")\n",
    "    \n",
    "    def standardize_column_names(self, df):\n",
    "        \"\"\"Standardize column names to lowercase with underscores.\"\"\"\n",
    "        original_cols = list(df.columns)\n",
    "        df.columns = df.columns.str.lower().str.strip().str.replace(' ', '_').str.replace('-', '_')\n",
    "        self.log(f\"Standardized {len(original_cols)} column names\")\n",
    "        return df\n",
    "    \n",
    "    def parse_dates(self, df, date_column='date'):\n",
    "        \"\"\"Parse date columns with multiple format support.\"\"\"\n",
    "        if date_column in df.columns:\n",
    "            # Try multiple date formats\n",
    "            for fmt in ['%d-%m-%Y', '%Y-%m-%d', '%d/%m/%Y', '%Y/%m/%d']:\n",
    "                try:\n",
    "                    df[date_column] = pd.to_datetime(df[date_column], format=fmt)\n",
    "                    self.log(f\"Parsed dates using format: {fmt}\")\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            else:\n",
    "                df[date_column] = pd.to_datetime(df[date_column], infer_datetime_format=True)\n",
    "                self.log(\"Parsed dates using inferred format\")\n",
    "        return df\n",
    "    \n",
    "    def standardize_geography(self, df):\n",
    "        \"\"\"Standardize state and district names.\"\"\"\n",
    "        if 'state' in df.columns:\n",
    "            df['state'] = df['state'].str.strip().str.title()\n",
    "            # Handle common variations\n",
    "            state_mapping = {\n",
    "                'Jammu And Kashmir': 'Jammu and Kashmir',\n",
    "                'Andaman And Nicobar Islands': 'Andaman and Nicobar Islands',\n",
    "                'Dadra And Nagar Haveli': 'Dadra and Nagar Haveli',\n",
    "                'Daman And Diu': 'Daman and Diu'\n",
    "            }\n",
    "            df['state'] = df['state'].replace(state_mapping)\n",
    "            self.log(f\"Standardized {df['state'].nunique()} unique states\")\n",
    "        \n",
    "        if 'district' in df.columns:\n",
    "            df['district'] = df['district'].str.strip().str.title()\n",
    "            self.log(f\"Standardized {df['district'].nunique()} unique districts\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def handle_pincode(self, df):\n",
    "        \"\"\"Validate and clean PIN codes.\"\"\"\n",
    "        if 'pincode' in df.columns:\n",
    "            df['pincode'] = df['pincode'].astype(str).str.strip()\n",
    "            df['pincode'] = df['pincode'].str.zfill(6)  # Ensure 6 digits\n",
    "            # Remove invalid pincodes\n",
    "            invalid_mask = ~df['pincode'].str.match(r'^[1-9][0-9]{5}$')\n",
    "            invalid_count = invalid_mask.sum()\n",
    "            if invalid_count > 0:\n",
    "                self.log(f\"Flagged {invalid_count} potentially invalid PIN codes\")\n",
    "            df['pincode_valid'] = ~invalid_mask\n",
    "        return df\n",
    "    \n",
    "    def remove_duplicates(self, df, subset=None):\n",
    "        \"\"\"Remove duplicate entries.\"\"\"\n",
    "        initial_count = len(df)\n",
    "        df = df.drop_duplicates(subset=subset)\n",
    "        removed = initial_count - len(df)\n",
    "        if removed > 0:\n",
    "            self.log(f\"Removed {removed:,} duplicate rows\")\n",
    "        return df\n",
    "    \n",
    "    def handle_missing_numeric(self, df, columns, method='median'):\n",
    "        \"\"\"Handle missing values in numeric columns.\"\"\"\n",
    "        for col in columns:\n",
    "            if col in df.columns:\n",
    "                missing_count = df[col].isnull().sum()\n",
    "                if missing_count > 0:\n",
    "                    if method == 'median':\n",
    "                        fill_value = df[col].median()\n",
    "                    elif method == 'mean':\n",
    "                        fill_value = df[col].mean()\n",
    "                    else:\n",
    "                        fill_value = 0\n",
    "                    df[col] = df[col].fillna(fill_value)\n",
    "                    self.log(f\"Filled {missing_count} missing values in '{col}' with {method}\")\n",
    "        return df\n",
    "    \n",
    "    def clean(self, df, dataset_name):\n",
    "        \"\"\"Execute full cleaning pipeline.\"\"\"\n",
    "        print(f\"\\nCleaning: {dataset_name}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        df = self.standardize_column_names(df)\n",
    "        df = self.parse_dates(df)\n",
    "        df = self.standardize_geography(df)\n",
    "        df = self.handle_pincode(df)\n",
    "        df = self.remove_duplicates(df)\n",
    "        \n",
    "        # Identify and handle numeric columns\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        df = self.handle_missing_numeric(df, numeric_cols)\n",
    "        \n",
    "        print(f\"   Final shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "        return df\n",
    "\n",
    "# Execute cleaning\n",
    "cleaner = DataCleaner()\n",
    "\n",
    "if df_enrol is not None:\n",
    "    df_enrol = cleaner.clean(df_enrol.copy(), \"Enrolment Dataset\")\n",
    "\n",
    "if df_demo is not None:\n",
    "    df_demo = cleaner.clean(df_demo.copy(), \"Demographic Update Dataset\")\n",
    "\n",
    "if df_bio is not None:\n",
    "    df_bio = cleaner.clean(df_bio.copy(), \"Biometric Update Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Merging and Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATASET INTEGRATION\n",
    "# =============================================================================\n",
    "\n",
    "def merge_datasets(df_enrol, df_demo, df_bio):\n",
    "    \"\"\"\n",
    "    Merge all three datasets on common keys.\n",
    "    Creates a unified analytical dataset.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"DATASET INTEGRATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Identify common merge keys\n",
    "    merge_keys = ['date', 'state', 'district', 'pincode']\n",
    "    \n",
    "    # Check which keys exist in all datasets\n",
    "    available_keys = []\n",
    "    for key in merge_keys:\n",
    "        exists_in_all = True\n",
    "        for df, name in [(df_enrol, 'enrol'), (df_demo, 'demo'), (df_bio, 'bio')]:\n",
    "            if df is not None and key not in df.columns:\n",
    "                exists_in_all = False\n",
    "                break\n",
    "        if exists_in_all:\n",
    "            available_keys.append(key)\n",
    "    \n",
    "    print(f\"Merge keys identified: {available_keys}\")\n",
    "    \n",
    "    # Start with enrolment as base\n",
    "    if df_enrol is not None:\n",
    "        df_merged = df_enrol.copy()\n",
    "        \n",
    "        # Rename enrolment columns for clarity\n",
    "        rename_map = {}\n",
    "        for col in df_merged.columns:\n",
    "            if col not in available_keys and 'enrol' not in col.lower():\n",
    "                if any(x in col.lower() for x in ['age', 'count', 'total']):\n",
    "                    rename_map[col] = f'enrol_{col}'\n",
    "        df_merged = df_merged.rename(columns=rename_map)\n",
    "        \n",
    "        # Merge demographic updates\n",
    "        if df_demo is not None and available_keys:\n",
    "            demo_cols = [col for col in df_demo.columns if col not in available_keys]\n",
    "            demo_rename = {col: f'demo_{col}' if 'demo' not in col.lower() else col for col in demo_cols}\n",
    "            df_demo_renamed = df_demo.rename(columns=demo_rename)\n",
    "            \n",
    "            df_merged = df_merged.merge(\n",
    "                df_demo_renamed,\n",
    "                on=available_keys,\n",
    "                how='outer',\n",
    "                suffixes=('', '_demo')\n",
    "            )\n",
    "            print(f\"Merged demographic data: {df_merged.shape}\")\n",
    "        \n",
    "        # Merge biometric updates\n",
    "        if df_bio is not None and available_keys:\n",
    "            bio_cols = [col for col in df_bio.columns if col not in available_keys]\n",
    "            bio_rename = {col: f'bio_{col}' if 'bio' not in col.lower() else col for col in bio_cols}\n",
    "            df_bio_renamed = df_bio.rename(columns=bio_rename)\n",
    "            \n",
    "            df_merged = df_merged.merge(\n",
    "                df_bio_renamed,\n",
    "                on=available_keys,\n",
    "                how='outer',\n",
    "                suffixes=('', '_bio')\n",
    "            )\n",
    "            print(f\"Merged biometric data: {df_merged.shape}\")\n",
    "        \n",
    "        # Fill NaN values with 0 for count columns\n",
    "        count_cols = [col for col in df_merged.columns if any(x in col.lower() for x in ['count', 'age', 'total'])]\n",
    "        df_merged[count_cols] = df_merged[count_cols].fillna(0)\n",
    "        \n",
    "        print(f\"\\nFinal merged dataset: {df_merged.shape[0]:,} rows x {df_merged.shape[1]} columns\")\n",
    "        return df_merged\n",
    "    \n",
    "    else:\n",
    "        print(\"Enrolment dataset required as base. Returning None.\")\n",
    "        return None\n",
    "\n",
    "# Execute merge\n",
    "df_master = merge_datasets(df_enrol, df_demo, df_bio)\n",
    "\n",
    "if df_master is not None:\n",
    "    print(\"\\n--- Merged Dataset Columns ---\")\n",
    "    print(list(df_master.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ALTERNATIVE: WORK WITH INDIVIDUAL DATASETS\n",
    "# =============================================================================\n",
    "# If datasets cannot be merged (different granularity), analyze them separately\n",
    "\n",
    "def create_working_dataset(df_enrol, df_demo, df_bio):\n",
    "    \"\"\"\n",
    "    Create working datasets when merge is not possible.\n",
    "    Aggregates to common granularity.\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    if df_enrol is not None:\n",
    "        datasets['enrolment'] = df_enrol\n",
    "    if df_demo is not None:\n",
    "        datasets['demographic'] = df_demo\n",
    "    if df_bio is not None:\n",
    "        datasets['biometric'] = df_bio\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Create fallback working datasets\n",
    "working_datasets = create_working_dataset(df_enrol, df_demo, df_bio)\n",
    "print(f\"\\nWorking datasets available: {list(working_datasets.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "Creating predictive features that capture infrastructure stress signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE ENGINEERING ENGINE\n",
    "# =============================================================================\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "    Creates analytical features for stress prediction.\n",
    "    \n",
    "    Key Features:\n",
    "    - Update Stress Ratio (USR): Ratio of updates to enrolments\n",
    "    - Age Transition Index (ATI): Child to adult transition pressure\n",
    "    - Volatility Score: Stability of update patterns\n",
    "    - Rolling Statistics: Trend indicators\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.features_created = []\n",
    "    \n",
    "    def identify_columns(self):\n",
    "        \"\"\"Identify relevant columns for feature engineering.\"\"\"\n",
    "        cols = self.df.columns.tolist()\n",
    "        \n",
    "        # Find enrolment columns\n",
    "        self.enrol_cols = [c for c in cols if 'enrol' in c.lower() and any(x in c.lower() for x in ['age', 'count', 'total'])]\n",
    "        \n",
    "        # Find demographic update columns\n",
    "        self.demo_cols = [c for c in cols if 'demo' in c.lower() and any(x in c.lower() for x in ['age', 'count', 'total', '5_17', '17_', '18'])]\n",
    "        \n",
    "        # Find biometric update columns\n",
    "        self.bio_cols = [c for c in cols if 'bio' in c.lower() and any(x in c.lower() for x in ['age', 'count', 'total', '5_17', '17_', '18'])]\n",
    "        \n",
    "        print(\"Identified columns:\")\n",
    "        print(f\"   Enrolment: {self.enrol_cols}\")\n",
    "        print(f\"   Demographic: {self.demo_cols}\")\n",
    "        print(f\"   Biometric: {self.bio_cols}\")\n",
    "    \n",
    "    def create_totals(self):\n",
    "        \"\"\"Create total columns for each category.\"\"\"\n",
    "        # Total enrolments\n",
    "        if self.enrol_cols:\n",
    "            numeric_enrol = [c for c in self.enrol_cols if self.df[c].dtype in ['int64', 'float64']]\n",
    "            if numeric_enrol:\n",
    "                self.df['total_enrolments'] = self.df[numeric_enrol].sum(axis=1)\n",
    "                self.features_created.append('total_enrolments')\n",
    "        \n",
    "        # Total demographic updates\n",
    "        if self.demo_cols:\n",
    "            numeric_demo = [c for c in self.demo_cols if self.df[c].dtype in ['int64', 'float64']]\n",
    "            if numeric_demo:\n",
    "                self.df['total_demo_updates'] = self.df[numeric_demo].sum(axis=1)\n",
    "                self.features_created.append('total_demo_updates')\n",
    "        \n",
    "        # Total biometric updates\n",
    "        if self.bio_cols:\n",
    "            numeric_bio = [c for c in self.bio_cols if self.df[c].dtype in ['int64', 'float64']]\n",
    "            if numeric_bio:\n",
    "                self.df['total_bio_updates'] = self.df[numeric_bio].sum(axis=1)\n",
    "                self.features_created.append('total_bio_updates')\n",
    "        \n",
    "        # Combined total updates\n",
    "        update_cols = ['total_demo_updates', 'total_bio_updates']\n",
    "        available_update_cols = [c for c in update_cols if c in self.df.columns]\n",
    "        if available_update_cols:\n",
    "            self.df['total_updates'] = self.df[available_update_cols].sum(axis=1)\n",
    "            self.features_created.append('total_updates')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_update_stress_ratio(self):\n",
    "        \"\"\"\n",
    "        Update Stress Ratio (USR)\n",
    "        = (Demographic Updates + Biometric Updates) / Enrolments\n",
    "        \n",
    "        High USR indicates infrastructure under update pressure.\n",
    "        \"\"\"\n",
    "        if 'total_updates' in self.df.columns and 'total_enrolments' in self.df.columns:\n",
    "            self.df['update_stress_ratio'] = np.where(\n",
    "                self.df['total_enrolments'] > 0,\n",
    "                self.df['total_updates'] / self.df['total_enrolments'],\n",
    "                0\n",
    "            )\n",
    "            self.features_created.append('update_stress_ratio')\n",
    "            print(\"   Created: Update Stress Ratio (USR)\")\n",
    "        return self\n",
    "    \n",
    "    def create_age_transition_index(self):\n",
    "        \"\"\"\n",
    "        Age Transition Index (ATI)\n",
    "        = bio_updates_18+ / demo_updates_5_17\n",
    "        \n",
    "        High ATI indicates child-to-adult transition pressure.\n",
    "        \"\"\"\n",
    "        # Find 18+ biometric column\n",
    "        bio_adult = [c for c in self.df.columns if 'bio' in c.lower() and ('17_' in c or '18' in c)]\n",
    "        # Find 5-17 demographic column\n",
    "        demo_child = [c for c in self.df.columns if 'demo' in c.lower() and '5_17' in c]\n",
    "        \n",
    "        if bio_adult and demo_child:\n",
    "            self.df['age_transition_index'] = np.where(\n",
    "                self.df[demo_child[0]] > 0,\n",
    "                self.df[bio_adult[0]] / self.df[demo_child[0]],\n",
    "                0\n",
    "            )\n",
    "            self.features_created.append('age_transition_index')\n",
    "            print(\"   Created: Age Transition Index (ATI)\")\n",
    "        return self\n",
    "    \n",
    "    def create_temporal_features(self):\n",
    "        \"\"\"Extract temporal features from date column.\"\"\"\n",
    "        if 'date' in self.df.columns:\n",
    "            self.df['year'] = self.df['date'].dt.year\n",
    "            self.df['month'] = self.df['date'].dt.month\n",
    "            self.df['week'] = self.df['date'].dt.isocalendar().week\n",
    "            self.df['day_of_week'] = self.df['date'].dt.dayofweek\n",
    "            self.df['quarter'] = self.df['date'].dt.quarter\n",
    "            self.df['is_weekend'] = (self.df['day_of_week'] >= 5).astype(int)\n",
    "            self.df['is_month_start'] = self.df['date'].dt.is_month_start.astype(int)\n",
    "            self.df['is_month_end'] = self.df['date'].dt.is_month_end.astype(int)\n",
    "            \n",
    "            self.features_created.extend(['year', 'month', 'week', 'day_of_week', 'quarter', \n",
    "                                          'is_weekend', 'is_month_start', 'is_month_end'])\n",
    "            print(\"   Created: Temporal features\")\n",
    "        return self\n",
    "    \n",
    "    def create_rolling_features(self, window_sizes=[7, 14, 30], group_cols=['state', 'district']):\n",
    "        \"\"\"\n",
    "        Create rolling statistics for trend analysis.\n",
    "        \"\"\"\n",
    "        if 'date' not in self.df.columns:\n",
    "            return self\n",
    "        \n",
    "        # Sort by date\n",
    "        self.df = self.df.sort_values('date')\n",
    "        \n",
    "        # Target columns for rolling stats\n",
    "        target_cols = ['total_updates', 'total_enrolments', 'update_stress_ratio']\n",
    "        available_targets = [c for c in target_cols if c in self.df.columns]\n",
    "        \n",
    "        for col in available_targets:\n",
    "            for window in window_sizes:\n",
    "                # Rolling mean\n",
    "                self.df[f'{col}_rolling_{window}d_mean'] = (\n",
    "                    self.df.groupby(group_cols)[col]\n",
    "                    .transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "                )\n",
    "                # Rolling std (volatility)\n",
    "                self.df[f'{col}_rolling_{window}d_std'] = (\n",
    "                    self.df.groupby(group_cols)[col]\n",
    "                    .transform(lambda x: x.rolling(window, min_periods=1).std())\n",
    "                )\n",
    "                self.features_created.extend([f'{col}_rolling_{window}d_mean', f'{col}_rolling_{window}d_std'])\n",
    "        \n",
    "        print(f\"   Created: Rolling features for windows {window_sizes}\")\n",
    "        return self\n",
    "    \n",
    "    def create_volatility_score(self):\n",
    "        \"\"\"\n",
    "        Volatility Score\n",
    "        Coefficient of variation of updates.\n",
    "        High volatility indicates unstable infrastructure.\n",
    "        \"\"\"\n",
    "        if 'total_updates_rolling_7d_std' in self.df.columns and 'total_updates_rolling_7d_mean' in self.df.columns:\n",
    "            self.df['volatility_score'] = np.where(\n",
    "                self.df['total_updates_rolling_7d_mean'] > 0,\n",
    "                self.df['total_updates_rolling_7d_std'] / self.df['total_updates_rolling_7d_mean'],\n",
    "                0\n",
    "            )\n",
    "            self.features_created.append('volatility_score')\n",
    "            print(\"   Created: Volatility Score\")\n",
    "        return self\n",
    "    \n",
    "    def create_stress_labels(self, usr_thresholds=(3, 7)):\n",
    "        \"\"\"\n",
    "        Create stress classification labels based on USR.\n",
    "        \n",
    "        Stable: USR < lower_threshold\n",
    "        Watch: lower_threshold <= USR < upper_threshold\n",
    "        Critical: USR >= upper_threshold\n",
    "        \"\"\"\n",
    "        if 'update_stress_ratio' in self.df.columns:\n",
    "            lower, upper = usr_thresholds\n",
    "            conditions = [\n",
    "                self.df['update_stress_ratio'] < lower,\n",
    "                (self.df['update_stress_ratio'] >= lower) & (self.df['update_stress_ratio'] < upper),\n",
    "                self.df['update_stress_ratio'] >= upper\n",
    "            ]\n",
    "            labels = ['Stable', 'Watch', 'Critical']\n",
    "            self.df['stress_level'] = np.select(conditions, labels, default='Unknown')\n",
    "            \n",
    "            # Numeric encoding\n",
    "            self.df['stress_level_encoded'] = self.df['stress_level'].map(\n",
    "                {'Stable': 0, 'Watch': 1, 'Critical': 2, 'Unknown': -1}\n",
    "            )\n",
    "            \n",
    "            self.features_created.extend(['stress_level', 'stress_level_encoded'])\n",
    "            print(f\"   Created: Stress labels (thresholds: {usr_thresholds})\")\n",
    "        return self\n",
    "    \n",
    "    def engineer(self):\n",
    "        \"\"\"Execute full feature engineering pipeline.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"FEATURE ENGINEERING\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        self.identify_columns()\n",
    "        print(\"\\nCreating features:\")\n",
    "        \n",
    "        self.create_totals()\n",
    "        self.create_update_stress_ratio()\n",
    "        self.create_age_transition_index()\n",
    "        self.create_temporal_features()\n",
    "        self.create_rolling_features()\n",
    "        self.create_volatility_score()\n",
    "        self.create_stress_labels()\n",
    "        \n",
    "        # Fill any remaining NaN from rolling operations\n",
    "        self.df = self.df.fillna(0)\n",
    "        \n",
    "        print(f\"\\nTotal features created: {len(self.features_created)}\")\n",
    "        print(f\"Final dataset shape: {self.df.shape}\")\n",
    "        \n",
    "        return self.df\n",
    "\n",
    "# Execute feature engineering\n",
    "if df_master is not None:\n",
    "    fe = FeatureEngineer(df_master)\n",
    "    df_featured = fe.engineer()\n",
    "else:\n",
    "    print(\"Using individual datasets for feature engineering...\")\n",
    "    # Process each dataset separately\n",
    "    featured_datasets = {}\n",
    "    for name, df in working_datasets.items():\n",
    "        if df is not None:\n",
    "            fe = FeatureEngineer(df)\n",
    "            featured_datasets[name] = fe.engineer()\n",
    "    df_featured = featured_datasets.get('enrolment') or featured_datasets.get('demographic') or featured_datasets.get('biometric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "if df_featured is not None:\n",
    "    print(\"=\"*70)\n",
    "    print(\"FEATURE SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Display key engineered features\n",
    "    key_features = ['update_stress_ratio', 'age_transition_index', 'volatility_score', 'stress_level']\n",
    "    available_features = [f for f in key_features if f in df_featured.columns]\n",
    "    \n",
    "    if available_features:\n",
    "        print(\"\\nKey Feature Statistics:\")\n",
    "        display(df_featured[available_features].describe())\n",
    "        \n",
    "        if 'stress_level' in df_featured.columns:\n",
    "            print(\"\\nStress Level Distribution:\")\n",
    "            print(df_featured['stress_level'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Comprehensive analysis including univariate, bivariate, and trivariate exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EDA ENGINE\n",
    "# =============================================================================\n",
    "\n",
    "class ExploratoryAnalysis:\n",
    "    \"\"\"Comprehensive EDA for Aadhaar datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.insights = []\n",
    "    \n",
    "    def add_insight(self, category, finding):\n",
    "        \"\"\"Record an analytical insight.\"\"\"\n",
    "        self.insights.append({'category': category, 'finding': finding})\n",
    "    \n",
    "    # =========================================================================\n",
    "    # UNIVARIATE ANALYSIS\n",
    "    # =========================================================================\n",
    "    \n",
    "    def univariate_numeric(self, column, figsize=(12, 4)):\n",
    "        \"\"\"Univariate analysis for numeric columns.\"\"\"\n",
    "        if column not in self.df.columns:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "        \n",
    "        # Histogram\n",
    "        axes[0].hist(self.df[column].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
    "        axes[0].set_title(f'Distribution: {column}')\n",
    "        axes[0].set_xlabel(column)\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        \n",
    "        # Box plot\n",
    "        axes[1].boxplot(self.df[column].dropna())\n",
    "        axes[1].set_title(f'Box Plot: {column}')\n",
    "        axes[1].set_ylabel(column)\n",
    "        \n",
    "        # Summary stats\n",
    "        stats_text = f\"Mean: {self.df[column].mean():.2f}\\n\"\n",
    "        stats_text += f\"Median: {self.df[column].median():.2f}\\n\"\n",
    "        stats_text += f\"Std: {self.df[column].std():.2f}\\n\"\n",
    "        stats_text += f\"Skew: {self.df[column].skew():.2f}\\n\"\n",
    "        stats_text += f\"Kurtosis: {self.df[column].kurtosis():.2f}\"\n",
    "        axes[2].text(0.1, 0.5, stats_text, transform=axes[2].transAxes, \n",
    "                     fontsize=12, verticalalignment='center', fontfamily='monospace')\n",
    "        axes[2].set_title(f'Statistics: {column}')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Record insight\n",
    "        skewness = self.df[column].skew()\n",
    "        if abs(skewness) > 1:\n",
    "            self.add_insight('Univariate', f'{column} is highly skewed ({skewness:.2f}), suggesting outliers or non-normal distribution.')\n",
    "    \n",
    "    def univariate_categorical(self, column, top_n=15, figsize=(12, 5)):\n",
    "        \"\"\"Univariate analysis for categorical columns.\"\"\"\n",
    "        if column not in self.df.columns:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # Value counts\n",
    "        value_counts = self.df[column].value_counts().head(top_n)\n",
    "        \n",
    "        # Bar chart\n",
    "        value_counts.plot(kind='barh', ax=axes[0], color='steelblue', edgecolor='black')\n",
    "        axes[0].set_title(f'Top {top_n} Categories: {column}')\n",
    "        axes[0].set_xlabel('Count')\n",
    "        axes[0].invert_yaxis()\n",
    "        \n",
    "        # Pie chart (top 10 only)\n",
    "        top_10 = value_counts.head(10)\n",
    "        axes[1].pie(top_10.values, labels=top_10.index, autopct='%1.1f%%', startangle=90)\n",
    "        axes[1].set_title(f'Proportion (Top 10): {column}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Record insight\n",
    "        top_category = value_counts.index[0]\n",
    "        top_pct = (value_counts.iloc[0] / len(self.df) * 100)\n",
    "        self.add_insight('Univariate', f'{column}: \"{top_category}\" dominates with {top_pct:.1f}% of records.')\n",
    "    \n",
    "    # =========================================================================\n",
    "    # BIVARIATE ANALYSIS\n",
    "    # =========================================================================\n",
    "    \n",
    "    def bivariate_scatter(self, x_col, y_col, hue_col=None, figsize=(10, 6)):\n",
    "        \"\"\"Scatter plot with optional grouping.\"\"\"\n",
    "        if x_col not in self.df.columns or y_col not in self.df.columns:\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        if hue_col and hue_col in self.df.columns:\n",
    "            for group in self.df[hue_col].unique():\n",
    "                subset = self.df[self.df[hue_col] == group]\n",
    "                ax.scatter(subset[x_col], subset[y_col], label=group, alpha=0.6)\n",
    "            ax.legend(title=hue_col)\n",
    "        else:\n",
    "            ax.scatter(self.df[x_col], self.df[y_col], alpha=0.6)\n",
    "        \n",
    "        ax.set_xlabel(x_col)\n",
    "        ax.set_ylabel(y_col)\n",
    "        ax.set_title(f'{y_col} vs {x_col}')\n",
    "        \n",
    "        # Add correlation\n",
    "        valid_data = self.df[[x_col, y_col]].dropna()\n",
    "        if len(valid_data) > 2:\n",
    "            corr, p_value = pearsonr(valid_data[x_col], valid_data[y_col])\n",
    "            ax.annotate(f'Correlation: {corr:.3f}', xy=(0.05, 0.95), xycoords='axes fraction')\n",
    "            self.add_insight('Bivariate', f'{x_col} vs {y_col}: Correlation = {corr:.3f}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def bivariate_boxplot(self, numeric_col, categorical_col, figsize=(12, 6)):\n",
    "        \"\"\"Box plots grouped by category.\"\"\"\n",
    "        if numeric_col not in self.df.columns or categorical_col not in self.df.columns:\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        # Limit categories if too many\n",
    "        top_categories = self.df[categorical_col].value_counts().head(15).index\n",
    "        plot_data = self.df[self.df[categorical_col].isin(top_categories)]\n",
    "        \n",
    "        plot_data.boxplot(column=numeric_col, by=categorical_col, ax=ax)\n",
    "        ax.set_title(f'{numeric_col} by {categorical_col}')\n",
    "        ax.set_xlabel(categorical_col)\n",
    "        ax.set_ylabel(numeric_col)\n",
    "        plt.suptitle('')  # Remove automatic title\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def correlation_matrix(self, columns=None, figsize=(12, 10)):\n",
    "        \"\"\"Correlation heatmap for numeric columns.\"\"\"\n",
    "        if columns is None:\n",
    "            columns = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        # Limit to 20 columns for readability\n",
    "        columns = columns[:20]\n",
    "        \n",
    "        corr_matrix = self.df[columns].corr()\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "                    center=0, square=True, linewidths=0.5, ax=ax, cbar_kws={'shrink': 0.8})\n",
    "        ax.set_title('Feature Correlation Matrix')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Find strong correlations\n",
    "        strong_corr = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                if abs(corr_matrix.iloc[i, j]) > 0.7:\n",
    "                    strong_corr.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "        \n",
    "        if strong_corr:\n",
    "            self.add_insight('Correlation', f'Found {len(strong_corr)} strong correlations (|r| > 0.7)')\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TRIVARIATE ANALYSIS\n",
    "    # =========================================================================\n",
    "    \n",
    "    def trivariate_heatmap(self, x_col, y_col, value_col, aggfunc='mean', figsize=(14, 10)):\n",
    "        \"\"\"Pivot table heatmap for three variables.\"\"\"\n",
    "        if not all(c in self.df.columns for c in [x_col, y_col, value_col]):\n",
    "            return\n",
    "        \n",
    "        # Create pivot table\n",
    "        pivot = self.df.pivot_table(values=value_col, index=y_col, columns=x_col, aggfunc=aggfunc)\n",
    "        \n",
    "        # Limit size\n",
    "        pivot = pivot.iloc[:20, :20]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        sns.heatmap(pivot, annot=True, fmt='.2f', cmap='YlOrRd', ax=ax)\n",
    "        ax.set_title(f'{aggfunc.title()} {value_col} by {x_col} and {y_col}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def trivariate_bubble(self, x_col, y_col, size_col, color_col=None, figsize=(12, 8)):\n",
    "        \"\"\"Bubble chart for four dimensions.\"\"\"\n",
    "        if not all(c in self.df.columns for c in [x_col, y_col, size_col]):\n",
    "            return\n",
    "        \n",
    "        # Sample if too large\n",
    "        plot_data = self.df.sample(min(1000, len(self.df)))\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        # Normalize size for visualization\n",
    "        sizes = (plot_data[size_col] - plot_data[size_col].min()) / (plot_data[size_col].max() - plot_data[size_col].min()) * 500 + 10\n",
    "        \n",
    "        if color_col and color_col in plot_data.columns:\n",
    "            scatter = ax.scatter(plot_data[x_col], plot_data[y_col], s=sizes, \n",
    "                                c=plot_data[color_col], cmap='viridis', alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "            plt.colorbar(scatter, ax=ax, label=color_col)\n",
    "        else:\n",
    "            ax.scatter(plot_data[x_col], plot_data[y_col], s=sizes, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "        \n",
    "        ax.set_xlabel(x_col)\n",
    "        ax.set_ylabel(y_col)\n",
    "        ax.set_title(f'{y_col} vs {x_col} (size: {size_col})')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TEMPORAL ANALYSIS\n",
    "    # =========================================================================\n",
    "    \n",
    "    def temporal_trend(self, date_col, value_col, group_col=None, figsize=(14, 6)):\n",
    "        \"\"\"Time series trend analysis.\"\"\"\n",
    "        if date_col not in self.df.columns or value_col not in self.df.columns:\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        if group_col and group_col in self.df.columns:\n",
    "            # Top groups only\n",
    "            top_groups = self.df[group_col].value_counts().head(5).index\n",
    "            for group in top_groups:\n",
    "                subset = self.df[self.df[group_col] == group].groupby(date_col)[value_col].mean()\n",
    "                ax.plot(subset.index, subset.values, label=group, marker='o', markersize=3)\n",
    "            ax.legend(title=group_col)\n",
    "        else:\n",
    "            trend_data = self.df.groupby(date_col)[value_col].mean()\n",
    "            ax.plot(trend_data.index, trend_data.values, marker='o', markersize=3)\n",
    "        \n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel(value_col)\n",
    "        ax.set_title(f'Temporal Trend: {value_col}')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # GEOGRAPHIC ANALYSIS\n",
    "    # =========================================================================\n",
    "    \n",
    "    def geographic_summary(self, geo_col='state', value_col='update_stress_ratio', figsize=(14, 8)):\n",
    "        \"\"\"Geographic distribution analysis.\"\"\"\n",
    "        if geo_col not in self.df.columns or value_col not in self.df.columns:\n",
    "            return\n",
    "        \n",
    "        # Aggregate by geography\n",
    "        geo_agg = self.df.groupby(geo_col).agg({\n",
    "            value_col: ['mean', 'std', 'count']\n",
    "        }).round(3)\n",
    "        geo_agg.columns = ['mean', 'std', 'count']\n",
    "        geo_agg = geo_agg.sort_values('mean', ascending=False).head(20)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # Bar chart of means\n",
    "        geo_agg['mean'].plot(kind='barh', ax=axes[0], color='steelblue', edgecolor='black')\n",
    "        axes[0].set_title(f'Mean {value_col} by {geo_col}')\n",
    "        axes[0].set_xlabel(f'Mean {value_col}')\n",
    "        axes[0].invert_yaxis()\n",
    "        \n",
    "        # Error bar chart\n",
    "        axes[1].barh(geo_agg.index, geo_agg['mean'], xerr=geo_agg['std'], color='coral', edgecolor='black', capsize=3)\n",
    "        axes[1].set_title(f'{value_col} with Std Dev by {geo_col}')\n",
    "        axes[1].set_xlabel(value_col)\n",
    "        axes[1].invert_yaxis()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Top stress locations\n",
    "        top_stress = geo_agg.head(3).index.tolist()\n",
    "        self.add_insight('Geographic', f'Highest {value_col} observed in: {\", \".join(top_stress)}')\n",
    "        \n",
    "        return geo_agg\n",
    "    \n",
    "    def get_insights(self):\n",
    "        \"\"\"Return all recorded insights.\"\"\"\n",
    "        return pd.DataFrame(self.insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXECUTE EXPLORATORY ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "if df_featured is not None:\n",
    "    eda = ExploratoryAnalysis(df_featured)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"UNIVARIATE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Analyze key features\n",
    "    if 'update_stress_ratio' in df_featured.columns:\n",
    "        eda.univariate_numeric('update_stress_ratio')\n",
    "    \n",
    "    if 'total_updates' in df_featured.columns:\n",
    "        eda.univariate_numeric('total_updates')\n",
    "    \n",
    "    if 'volatility_score' in df_featured.columns:\n",
    "        eda.univariate_numeric('volatility_score')\n",
    "    \n",
    "    if 'state' in df_featured.columns:\n",
    "        eda.univariate_categorical('state')\n",
    "    \n",
    "    if 'stress_level' in df_featured.columns:\n",
    "        eda.univariate_categorical('stress_level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BIVARIATE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "if df_featured is not None:\n",
    "    print(\"=\"*70)\n",
    "    print(\"BIVARIATE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Enrolments vs Updates\n",
    "    if 'total_enrolments' in df_featured.columns and 'total_updates' in df_featured.columns:\n",
    "        eda.bivariate_scatter('total_enrolments', 'total_updates', hue_col='stress_level' if 'stress_level' in df_featured.columns else None)\n",
    "    \n",
    "    # Stress ratio by state\n",
    "    if 'update_stress_ratio' in df_featured.columns and 'state' in df_featured.columns:\n",
    "        eda.bivariate_boxplot('update_stress_ratio', 'state')\n",
    "    \n",
    "    # Correlation matrix\n",
    "    key_numeric = ['total_enrolments', 'total_updates', 'total_demo_updates', 'total_bio_updates',\n",
    "                   'update_stress_ratio', 'age_transition_index', 'volatility_score']\n",
    "    available_numeric = [c for c in key_numeric if c in df_featured.columns]\n",
    "    if available_numeric:\n",
    "        eda.correlation_matrix(available_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRIVARIATE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "if df_featured is not None:\n",
    "    print(\"=\"*70)\n",
    "    print(\"TRIVARIATE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # State x Month x Stress Ratio\n",
    "    if 'state' in df_featured.columns and 'month' in df_featured.columns and 'update_stress_ratio' in df_featured.columns:\n",
    "        eda.trivariate_heatmap('month', 'state', 'update_stress_ratio')\n",
    "    \n",
    "    # Bubble chart: Enrolments vs Updates, sized by volatility\n",
    "    if all(c in df_featured.columns for c in ['total_enrolments', 'total_updates', 'volatility_score']):\n",
    "        color_col = 'stress_level_encoded' if 'stress_level_encoded' in df_featured.columns else None\n",
    "        eda.trivariate_bubble('total_enrolments', 'total_updates', 'volatility_score', color_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEMPORAL AND GEOGRAPHIC ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "if df_featured is not None:\n",
    "    print(\"=\"*70)\n",
    "    print(\"TEMPORAL & GEOGRAPHIC ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Temporal trends\n",
    "    if 'date' in df_featured.columns and 'update_stress_ratio' in df_featured.columns:\n",
    "        eda.temporal_trend('date', 'update_stress_ratio', group_col='state' if 'state' in df_featured.columns else None)\n",
    "    \n",
    "    # Geographic summary\n",
    "    if 'state' in df_featured.columns and 'update_stress_ratio' in df_featured.columns:\n",
    "        state_summary = eda.geographic_summary('state', 'update_stress_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EDA INSIGHTS SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "if df_featured is not None:\n",
    "    print(\"=\"*70)\n",
    "    print(\"KEY INSIGHTS FROM EDA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    insights_df = eda.get_insights()\n",
    "    if not insights_df.empty:\n",
    "        for idx, row in insights_df.iterrows():\n",
    "            print(f\"\\n[{row['category']}]\")\n",
    "            print(f\"   {row['finding']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predictive Modeling Layer\n",
    "\n",
    "Three complementary models:\n",
    "1. Stress Classification Model (XGBoost/Random Forest)\n",
    "2. Time-Series Forecasting (for trend prediction)\n",
    "3. Anomaly Detection (Isolation Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL 1: STRESS CLASSIFICATION\n",
    "# =============================================================================\n",
    "\n",
    "class StressClassifier:\n",
    "    \"\"\"\n",
    "    Classifies each district/pincode/time period as:\n",
    "    - Stable (low stress)\n",
    "    - Watch (moderate stress)  \n",
    "    - Critical (high stress)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_importance = None\n",
    "        self.metrics = {}\n",
    "    \n",
    "    def prepare_features(self):\n",
    "        \"\"\"Prepare feature matrix and target variable.\"\"\"\n",
    "        # Define potential features\n",
    "        potential_features = [\n",
    "            'total_enrolments', 'total_updates', 'total_demo_updates', 'total_bio_updates',\n",
    "            'age_transition_index', 'volatility_score',\n",
    "            'total_updates_rolling_7d_mean', 'total_updates_rolling_7d_std',\n",
    "            'total_updates_rolling_14d_mean', 'total_updates_rolling_30d_mean',\n",
    "            'month', 'quarter', 'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end'\n",
    "        ]\n",
    "        \n",
    "        # Select available features\n",
    "        self.feature_cols = [c for c in potential_features if c in self.df.columns]\n",
    "        \n",
    "        # Check for target\n",
    "        if 'stress_level_encoded' not in self.df.columns:\n",
    "            print(\"Error: stress_level_encoded not found. Run feature engineering first.\")\n",
    "            return None, None\n",
    "        \n",
    "        # Create feature matrix\n",
    "        X = self.df[self.feature_cols].copy()\n",
    "        y = self.df['stress_level_encoded'].copy()\n",
    "        \n",
    "        # Remove invalid targets\n",
    "        valid_mask = y >= 0\n",
    "        X = X[valid_mask]\n",
    "        y = y[valid_mask]\n",
    "        \n",
    "        # Handle any remaining missing values\n",
    "        X = X.fillna(0)\n",
    "        \n",
    "        print(f\"Features selected: {len(self.feature_cols)}\")\n",
    "        print(f\"Training samples: {len(X):,}\")\n",
    "        print(f\"Class distribution:\")\n",
    "        print(y.value_counts().sort_index())\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def train(self, test_size=0.2, random_state=42):\n",
    "        \"\"\"Train the stress classification model.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"MODEL 1: STRESS CLASSIFICATION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        X, y = self.prepare_features()\n",
    "        if X is None:\n",
    "            return self\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Train Gradient Boosting Classifier\n",
    "        print(\"\\nTraining Gradient Boosting Classifier...\")\n",
    "        self.model = GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        self.model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = self.model.predict(X_test_scaled)\n",
    "        y_pred_proba = self.model.predict_proba(X_test_scaled)\n",
    "        \n",
    "        # Metrics\n",
    "        print(\"\\n--- Classification Report ---\")\n",
    "        target_names = ['Stable', 'Watch', 'Critical']\n",
    "        print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(self.model, X_train_scaled, y_train, cv=5, scoring='f1_weighted')\n",
    "        print(f\"\\nCross-validation F1 (weighted): {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})\")\n",
    "        \n",
    "        self.metrics = {\n",
    "            'cv_f1_mean': cv_scores.mean(),\n",
    "            'cv_f1_std': cv_scores.std(),\n",
    "            'test_accuracy': (y_pred == y_test).mean()\n",
    "        }\n",
    "        \n",
    "        # Feature importance\n",
    "        self.feature_importance = pd.DataFrame({\n",
    "            'feature': self.feature_cols,\n",
    "            'importance': self.model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Store test data for visualization\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.y_pred = y_pred\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"Visualize model results.\"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"Model not trained yet.\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(self.y_test, self.y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "                    xticklabels=['Stable', 'Watch', 'Critical'],\n",
    "                    yticklabels=['Stable', 'Watch', 'Critical'])\n",
    "        axes[0].set_title('Confusion Matrix')\n",
    "        axes[0].set_xlabel('Predicted')\n",
    "        axes[0].set_ylabel('Actual')\n",
    "        \n",
    "        # Feature Importance\n",
    "        top_features = self.feature_importance.head(10)\n",
    "        axes[1].barh(top_features['feature'], top_features['importance'], color='steelblue', edgecolor='black')\n",
    "        axes[1].set_title('Top 10 Feature Importance')\n",
    "        axes[1].set_xlabel('Importance')\n",
    "        axes[1].invert_yaxis()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self, new_data):\n",
    "        \"\"\"Predict stress level for new data.\"\"\"\n",
    "        if self.model is None:\n",
    "            return None\n",
    "        \n",
    "        X_new = new_data[self.feature_cols].fillna(0)\n",
    "        X_new_scaled = self.scaler.transform(X_new)\n",
    "        \n",
    "        predictions = self.model.predict(X_new_scaled)\n",
    "        probabilities = self.model.predict_proba(X_new_scaled)\n",
    "        \n",
    "        result = new_data.copy()\n",
    "        result['predicted_stress'] = ['Stable', 'Watch', 'Critical'][predictions[0]] if len(predictions) == 1 else [['Stable', 'Watch', 'Critical'][p] for p in predictions]\n",
    "        result['confidence'] = probabilities.max(axis=1)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Train stress classifier\n",
    "if df_featured is not None and 'stress_level_encoded' in df_featured.columns:\n",
    "    stress_classifier = StressClassifier(df_featured)\n",
    "    stress_classifier.train()\n",
    "    stress_classifier.plot_results()\n",
    "else:\n",
    "    print(\"Stress classification requires engineered features. Skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL 2: TIME-SERIES FORECASTING\n",
    "# =============================================================================\n",
    "\n",
    "class UpdateForecaster:\n",
    "    \"\"\"\n",
    "    Forecasts future update load using time-series methods.\n",
    "    Predicts expected biometric & demographic update surges.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, target_col='total_updates', date_col='date'):\n",
    "        self.df = df\n",
    "        self.target_col = target_col\n",
    "        self.date_col = date_col\n",
    "        self.forecast = None\n",
    "    \n",
    "    def prepare_series(self, group_col=None, group_value=None):\n",
    "        \"\"\"Prepare time series data.\"\"\"\n",
    "        if self.date_col not in self.df.columns or self.target_col not in self.df.columns:\n",
    "            print(f\"Required columns not found: {self.date_col}, {self.target_col}\")\n",
    "            return None\n",
    "        \n",
    "        df_ts = self.df.copy()\n",
    "        \n",
    "        # Filter by group if specified\n",
    "        if group_col and group_value:\n",
    "            df_ts = df_ts[df_ts[group_col] == group_value]\n",
    "        \n",
    "        # Aggregate to daily level\n",
    "        ts_data = df_ts.groupby(self.date_col)[self.target_col].sum().reset_index()\n",
    "        ts_data = ts_data.set_index(self.date_col).sort_index()\n",
    "        \n",
    "        return ts_data\n",
    "    \n",
    "    def analyze_stationarity(self, ts_data):\n",
    "        \"\"\"Test for stationarity using ADF test.\"\"\"\n",
    "        if ts_data is None or len(ts_data) < 20:\n",
    "            return None\n",
    "        \n",
    "        result = adfuller(ts_data[self.target_col].dropna())\n",
    "        \n",
    "        print(\"\\n--- Stationarity Test (ADF) ---\")\n",
    "        print(f\"ADF Statistic: {result[0]:.4f}\")\n",
    "        print(f\"p-value: {result[1]:.4f}\")\n",
    "        print(f\"Stationary: {'Yes' if result[1] < 0.05 else 'No (differencing may be needed)'}\")\n",
    "        \n",
    "        return result[1] < 0.05\n",
    "    \n",
    "    def decompose(self, ts_data, period=7):\n",
    "        \"\"\"Decompose time series into trend, seasonal, and residual components.\"\"\"\n",
    "        if ts_data is None or len(ts_data) < period * 2:\n",
    "            print(\"Insufficient data for decomposition.\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            decomposition = seasonal_decompose(ts_data[self.target_col], model='additive', period=period)\n",
    "            \n",
    "            fig, axes = plt.subplots(4, 1, figsize=(14, 10))\n",
    "            \n",
    "            decomposition.observed.plot(ax=axes[0], title='Observed')\n",
    "            decomposition.trend.plot(ax=axes[1], title='Trend')\n",
    "            decomposition.seasonal.plot(ax=axes[2], title='Seasonal')\n",
    "            decomposition.resid.plot(ax=axes[3], title='Residual')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            return decomposition\n",
    "        except Exception as e:\n",
    "            print(f\"Decomposition error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def simple_forecast(self, ts_data, horizon=30):\n",
    "        \"\"\"\n",
    "        Simple forecasting using exponential smoothing.\n",
    "        For more advanced forecasting, consider Prophet or LSTM.\n",
    "        \"\"\"\n",
    "        if ts_data is None or len(ts_data) < 14:\n",
    "            print(\"Insufficient data for forecasting.\")\n",
    "            return None\n",
    "        \n",
    "        # Calculate rolling statistics for baseline forecast\n",
    "        recent_mean = ts_data[self.target_col].tail(14).mean()\n",
    "        recent_std = ts_data[self.target_col].tail(14).std()\n",
    "        trend = (ts_data[self.target_col].tail(7).mean() - ts_data[self.target_col].tail(14).head(7).mean()) / 7\n",
    "        \n",
    "        # Generate forecast dates\n",
    "        last_date = ts_data.index[-1]\n",
    "        forecast_dates = pd.date_range(start=last_date + timedelta(days=1), periods=horizon)\n",
    "        \n",
    "        # Simple linear trend forecast with noise\n",
    "        forecast_values = [recent_mean + trend * i for i in range(1, horizon + 1)]\n",
    "        \n",
    "        forecast_df = pd.DataFrame({\n",
    "            'date': forecast_dates,\n",
    "            'forecast': forecast_values,\n",
    "            'lower_bound': [v - 1.96 * recent_std for v in forecast_values],\n",
    "            'upper_bound': [v + 1.96 * recent_std for v in forecast_values]\n",
    "        })\n",
    "        \n",
    "        self.forecast = forecast_df\n",
    "        \n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "        \n",
    "        # Historical\n",
    "        ax.plot(ts_data.index, ts_data[self.target_col], label='Historical', color='blue')\n",
    "        \n",
    "        # Forecast\n",
    "        ax.plot(forecast_df['date'], forecast_df['forecast'], label='Forecast', color='red', linestyle='--')\n",
    "        ax.fill_between(forecast_df['date'], forecast_df['lower_bound'], forecast_df['upper_bound'],\n",
    "                        color='red', alpha=0.2, label='95% CI')\n",
    "        \n",
    "        ax.set_title(f'{horizon}-Day Forecast: {self.target_col}')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel(self.target_col)\n",
    "        ax.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return forecast_df\n",
    "\n",
    "# Run time series analysis\n",
    "if df_featured is not None and 'date' in df_featured.columns:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL 2: TIME-SERIES FORECASTING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    target_col = 'total_updates' if 'total_updates' in df_featured.columns else 'total_enrolments' if 'total_enrolments' in df_featured.columns else None\n",
    "    \n",
    "    if target_col:\n",
    "        forecaster = UpdateForecaster(df_featured, target_col=target_col)\n",
    "        ts_data = forecaster.prepare_series()\n",
    "        \n",
    "        if ts_data is not None and len(ts_data) > 14:\n",
    "            forecaster.analyze_stationarity(ts_data)\n",
    "            forecaster.decompose(ts_data)\n",
    "            forecast = forecaster.simple_forecast(ts_data, horizon=30)\n",
    "        else:\n",
    "            print(\"Insufficient time series data.\")\n",
    "    else:\n",
    "        print(\"No suitable target column for forecasting.\")\n",
    "else:\n",
    "    print(\"Time series forecasting requires date column. Skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL 3: ANOMALY DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "class AnomalyDetector:\n",
    "    \"\"\"\n",
    "    Detects silent failures and unusual patterns.\n",
    "    Uses Isolation Forest for unsupervised anomaly detection.\n",
    "    \n",
    "    Detects:\n",
    "    - PIN codes with sudden update spikes\n",
    "    - Machine failures\n",
    "    - Repeated correction loops\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.model = None\n",
    "        self.anomalies = None\n",
    "    \n",
    "    def prepare_features(self):\n",
    "        \"\"\"Prepare features for anomaly detection.\"\"\"\n",
    "        potential_features = [\n",
    "            'total_updates', 'total_enrolments', 'update_stress_ratio',\n",
    "            'age_transition_index', 'volatility_score',\n",
    "            'total_updates_rolling_7d_mean', 'total_updates_rolling_7d_std'\n",
    "        ]\n",
    "        \n",
    "        self.feature_cols = [c for c in potential_features if c in self.df.columns]\n",
    "        \n",
    "        
Continue

10:26 PM
<function_results>Created file /home/claude/APIx_Aadhaar_Predictive_Intelligence.ipynb (21,078 bytes)

continue

10:26 PM
Failed to edit
APIx Aadhaar Predictive Intelligence
Let me recreate the complete notebook:


Creating the complete Jupyter notebook for Aadhaar Predictive Intelligence hackathon
APIx_Aadhaar_Hackathon.ipynb
/home/claude/APIx_Aadhaar_Hackathon.ipynb

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIx: Aadhaar Predictive Intelligence and Early-Warning System\n",
    "\n",
    "**A Data-Driven Early Warning System for Aadhaar Infrastructure Stress Prediction**\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This notebook implements a comprehensive analytical pipeline that transforms raw Aadhaar enrolment and update data into actionable intelligence. The system predicts where Aadhaar infrastructure will face stress before citizens experience delays or failures.\n",
    "\n",
    "**Core Components:**\n",
    "1. Data Ingestion and Cleaning\n",
    "2. Feature Engineering (Stress Ratios, Age Transition Indices, Volatility Scores)\n",
    "3. Exploratory Data Analysis (Univariate, Bivariate, Trivariate)\n",
    "4. Predictive Modeling (Classification, Time-Series Forecasting, Anomaly Detection)\n",
    "5. LLM-Powered Explainability Layer\n",
    "6. Decision Framework and Recommendations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install pandas numpy matplotlib seaborn plotly scikit-learn statsmodels scipy openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Time Series\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# LLM Integration\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Display Settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Plot Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Environment configured successfully.\")\n",
    "print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Ingestion Layer\n",
    "\n",
    "Load the three core datasets:\n",
    "- Aadhaar Enrolment Dataset\n",
    "- Aadhaar Demographic Update Dataset  \n",
    "- Aadhaar Biometric Update Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING CONFIGURATION\n",
    "# =============================================================================\n",
    "# UPDATE THESE PATHS TO MATCH YOUR DATASET LOCATIONS\n",
    "\n",
    "ENROLMENT_PATH = 'aadhaar_enrolment.csv'              # Path to enrolment dataset\n",
    "DEMOGRAPHIC_PATH = 'aadhaar_demographic_update.csv'   # Path to demographic update dataset\n",
    "BIOMETRIC_PATH = 'aadhaar_biometric_update.csv'       # Path to biometric update dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD DATASETS\n",
    "# =============================================================================\n",
    "\n",
    "def load_dataset(filepath, dataset_name):\n",
    "    \"\"\"Load dataset with error handling and initial diagnostics.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"[SUCCESS] {dataset_name}\")\n",
    "        print(f\"   Shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "        print(f\"   Columns: {list(df.columns)}\")\n",
    "        print()\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] {dataset_name} not found at: {filepath}\")\n",
    "        print(\"   Please update the path variable above.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load {dataset_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATA INGESTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_enrol = load_dataset(ENROLMENT_PATH, \"Enrolment Dataset\")\n",
    "df_demo = load_dataset(DEMOGRAPHIC_PATH, \"Demographic Update Dataset\")\n",
    "df_bio = load_dataset(BIOMETRIC_PATH, \"Biometric Update Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INITIAL DATA INSPECTION\n",
    "# =============================================================================\n",
    "\n",
    "def inspect_dataset(df, name):\n",
    "    \"\"\"Comprehensive initial inspection of dataset.\"\"\"\n",
    "    if df is None:\n",
    "        print(f\"Dataset {name} not loaded.\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"INSPECTION: {name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n--- First 5 Rows ---\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\n--- Data Types ---\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\n--- Missing Values ---\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df) * 100).round(2)\n",
    "    missing_df = pd.DataFrame({'Missing': missing, 'Percentage': missing_pct})\n",
    "    print(missing_df[missing_df['Missing'] > 0] if missing.sum() > 0 else \"No missing values detected.\")\n",
    "    \n",
    "    print(\"\\n--- Statistical Summary ---\")\n",
    "    display(df.describe())\n",
    "    print()\n",
    "\n",
    "inspect_dataset(df_enrol, \"Enrolment Dataset\")\n",
    "inspect_dataset(df_demo, \"Demographic Update Dataset\")\n",
    "inspect_dataset(df_bio, \"Biometric Update Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA CLEANING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "class DataCleaner:\n",
    "    \"\"\"Comprehensive data cleaning pipeline for Aadhaar datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cleaning_log = []\n",
    "    \n",
    "    def log(self, message):\n",
    "        \"\"\"Log cleaning operations.\"\"\"\n",
    "        timestamp = datetime.now().strftime('%H:%M:%S')\n",
    "        self.cleaning_log.append(f\"[{timestamp}] {message}\")\n",
    "        print(f\"   {message}\")\n",
    "    \n",
    "    def standardize_column_names(self, df):\n",
    "        \"\"\"Standardize column names to lowercase with underscores.\"\"\"\n",
    "        original_cols = list(df.columns)\n",
    "        df.columns = df.columns.str.lower().str.strip().str.replace(' ', '_').str.replace('-', '_')\n",
    "        self.log(f\"Standardized {len(original_cols)} column names\")\n",
    "        return df\n",
    "    \n",
    "    def parse_dates(self, df, date_column='date'):\n",
    "        \"\"\"Parse date columns with multiple format support.\"\"\"\n",
    "        if date_column in df.columns:\n",
    "            for fmt in ['%d-%m-%Y', '%Y-%m-%d', '%d/%m/%Y', '%Y/%m/%d']:\n",
    "                try:\n",
    "                    df[date_column] = pd.to_datetime(df[date_column], format=fmt)\n",
    "                    self.log(f\"Parsed dates using format: {fmt}\")\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            else:\n",
    "                df[date_column] = pd.to_datetime(df[date_column], infer_datetime_format=True)\n",
    "                self.log(\"Parsed dates using inferred format\")\n",
    "        return df\n",
    "    \n",
    "    def standardize_geography(self, df):\n",
    "        \"\"\"Standardize state and district names.\"\"\"\n",
    "        if 'state' in df.columns:\n",
    "            df['state'] = df['state'].str.strip().str.title()\n",
    "            state_mapping = {\n",
    "                'Jammu And Kashmir': 'Jammu and Kashmir',\n",
    "                'Andaman And Nicobar Islands': 'Andaman and Nicobar Islands',\n",
    "                'Dadra And Nagar Haveli': 'Dadra and Nagar Haveli',\n",
    "                'Daman And Diu': 'Daman and Diu'\n",
    "            }\n",
    "            df['state'] = df['state'].replace(state_mapping)\n",
    "            self.log(f\"Standardized {df['state'].nunique()} unique states\")\n",
    "        \n",
    "        if 'district' in df.columns:\n",
    "            df['district'] = df['district'].str.strip().str.title()\n",
    "            self.log(f\"Standardized {df['district'].nunique()} unique districts\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def handle_pincode(self, df):\n",
    "        \"\"\"Validate and clean PIN codes.\"\"\"\n",
    "        if 'pincode' in df.columns:\n",
    "            df['pincode'] = df['pincode'].astype(str).str.strip()\n",
    "            df['pincode'] = df['pincode'].str.zfill(6)\n",
    "            invalid_mask = ~df['pincode'].str.match(r'^[1-9][0-9]{5}$')\n",
    "            invalid_count = invalid_mask.sum()\n",
    "            if invalid_count > 0:\n",
    "                self.log(f\"Flagged {invalid_count} potentially invalid PIN codes\")\n",
    "            df['pincode_valid'] = ~invalid_mask\n",
    "        return df\n",
    "    \n",
    "    def remove_duplicates(self, df, subset=None):\n",
    "        \"\"\"Remove duplicate entries.\"\"\"\n",
    "        initial_count = len(df)\n",
    "        df = df.drop_duplicates(subset=subset)\n",
    "        removed = initial_count - len(df)\n",
    "        if removed > 0:\n",
    "            self.log(f\"Removed {removed:,} duplicate rows\")\n",
    "        return df\n",
    "    \n",
    "    def handle_missing_numeric(self, df, columns, method='median'):\n",
    "        \"\"\"Handle missing values in numeric columns.\"\"\"\n",
    "        for col in columns:\n",
    "            if col in df.columns:\n",
    "                missing_count = df[col].isnull().sum()\n",
    "                if missing_count > 0:\n",
    "                    if method == 'median':\n",
    "                        fill_value = df[col].median()\n",
    "                    elif method == 'mean':\n",
    "                        fill_value = df[col].mean()\n",
    "                    else:\n",
    "                        fill_value = 0\n",
    "                    df[col] = df[col].fillna(fill_value)\n",
    "                    self.log(f\"Filled {missing_count} missing values in '{col}' with {method}\")\n",
    "        return df\n",
    "    \n",
    "    def clean(self, df, dataset_name):\n",
    "        \"\"\"Execute full cleaning pipeline.\"\"\"\n",
    "        print(f\"\\nCleaning: {dataset_name}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        df = self.standardize_column_names(df)\n",
    "        df = self.parse_dates(df)\n",
    "        df = self.standardize_geography(df)\n",
    "        df = self.handle_pincode(df)\n",
    "        df = self.remove_duplicates(df)\n",
    "        \n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        df = self.handle_missing_numeric(df, numeric_cols)\n",
    "        \n",
    "        print(f\"   Final shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "        return df\n",
    "\n",
    "# Execute cleaning\n",
    "cleaner = DataCleaner()\n",
    "\n",
    "if df_enrol is not None:\n",
    "    df_enrol = cleaner.clean(df_enrol.copy(), \"Enrolment Dataset\")\n",
    "\n",
    "if df_demo is not None:\n",
    "    df_demo = cleaner.clean(df_demo.copy(), \"Demographic Update Dataset\")\n",
    "\n",
    "if df_bio is not None:\n",
    "    df_bio = cleaner.clean(df_bio.copy(), \"Biometric Update Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Merging and Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATASET INTEGRATION\n",
    "# =============================================================================\n",
    "\n",
    "def merge_datasets(df_enrol, df_demo, df_bio):\n",
    "    \"\"\"\n",
    "    Merge all three datasets on common keys.\n",
    "    Creates a unified analytical dataset.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"DATASET INTEGRATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    merge_keys = ['date', 'state', 'district', 'pincode']\n",
    "    \n",
    "    # Check which keys exist in all datasets\n",
    "    available_keys = []\n",
    "    datasets = [(df_enrol, 'enrol'), (df_demo, 'demo'), (df_bio, 'bio')]\n",
    "    datasets = [(df, name) for df, name in datasets if df is not None]\n",
    "    \n",
    "    for key in merge_keys:\n",
    "        if all(key in df.columns for df, _ in datasets):\n",
    "            available_keys.append(key)\n",
    "    \n",
    "    print(f\"Merge keys identified: {available_keys}\")\n",
    "    \n",
    "    if df_enrol is not None:\n",
    "        df_merged = df_enrol.copy()\n",
    "        \n",
    "        # Prefix enrolment columns\n",
    "        rename_map = {}\n",
    "        for col in df_merged.columns:\n",
    "            if col not in available_keys and 'enrol' not in col.lower():\n",
    "                if any(x in col.lower() for x in ['age', 'count', 'total']):\n",
    "                    rename_map[col] = f'enrol_{col}'\n",
    "        df_merged = df_merged.rename(columns=rename_map)\n",
    "        \n",
    "        # Merge demographic updates\n",
    "        if df_demo is not None and available_keys:\n",
    "            demo_cols = [col for col in df_demo.columns if col not in available_keys]\n",
    "            demo_rename = {col: f'demo_{col}' if 'demo' not in col.lower() else col for col in demo_cols}\n",
    "            df_demo_renamed = df_demo.rename(columns=demo_rename)\n",
    "            \n",
    "            df_merged = df_merged.merge(\n",
    "                df_demo_renamed,\n",
    "                on=available_keys,\n",
    "                how='outer',\n",
    "                suffixes=('', '_demo')\n",
    "            )\n",
    "            print(f\"Merged demographic data: {df_merged.shape}\")\n",
    "        \n",
    "        # Merge biometric updates\n",
    "        if df_bio is not None and available_keys:\n",
    "            bio_cols = [col for col in df_bio.columns if col not in available_keys]\n",
    "            bio_rename = {col: f'bio_{col}' if 'bio' not in col.lower() else col for col in bio_cols}\n",
    "            df_bio_renamed = df_bio.rename(columns=bio_rename)\n",
    "            \n",
    "            df_merged = df_merged.merge(\n",
    "                df_bio_renamed,\n",
    "                on=available_keys,\n",
    "                how='outer',\n",
    "                suffixes=('', '_bio')\n",
    "            )\n",
    "            print(f\"Merged biometric data: {df_merged.shape}\")\n",
    "        \n",
    "        # Fill NaN values with 0 for count columns\n",
    "        count_cols = [col for col in df_merged.columns if any(x in col.lower() for x in ['count', 'age', 'total'])]\n",
    "        df_merged[count_cols] = df_merged[count_cols].fillna(0)\n",
    "        \n",
    "        print(f\"\\nFinal merged dataset: {df_merged.shape[0]:,} rows x {df_merged.shape[1]} columns\")\n",
    "        return df_merged\n",
    "    \n",
    "    # Fallback if no enrolment data\n",
    "    if df_demo is not None:\n",
    "        return df_demo\n",
    "    if df_bio is not None:\n",
    "        return df_bio\n",
    "    return None\n",
    "\n",
    "# Execute merge\n",
    "df_master = merge_datasets(df_enrol, df_demo, df_bio)\n",
    "\n",
    "if df_master is not None:\n",
    "    print(\"\\n--- Merged Dataset Columns ---\")\n",
    "    print(list(df_master.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "Creating predictive features that capture infrastructure stress signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE ENGINEERING ENGINE\n",
    "# =============================================================================\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "    Creates analytical features for stress prediction.\n",
    "    \n",
    "    Key Features:\n",
    "    - Update Stress Ratio (USR): Ratio of updates to enrolments\n",
    "    - Age Transition Index (ATI): Child to adult transition pressure\n",
    "    - Volatility Score: Stability of update patterns\n",
    "    - Rolling Statistics: Trend indicators\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.features_created = []\n",
    "        self.enrol_cols = []\n",
    "        self.demo_cols = []\n",
    "        self.bio_cols = []\n",
    "    \n",
    "    def identify_columns(self):\n",
    "        \"\"\"Identify relevant columns for feature engineering.\"\"\"\n",
    "        cols = self.df.columns.tolist()\n",
    "        \n",
    "        self.enrol_cols = [c for c in cols if 'enrol' in c.lower() and any(x in c.lower() for x in ['age', 'count', 'total'])]\n",
    "        self.demo_cols = [c for c in cols if 'demo' in c.lower() and any(x in c.lower() for x in ['age', 'count', 'total', '5_17', '17_', '18'])]\n",
    "        self.bio_cols = [c for c in cols if 'bio' in c.lower() and any(x in c.lower() for x in ['age', 'count', 'total', '5_17', '17_', '18'])]\n",
    "        \n",
    "        print(\"Identified columns:\")\n",
    "        print(f\"   Enrolment: {self.enrol_cols}\")\n",
    "        print(f\"   Demographic: {self.demo_cols}\")\n",
    "        print(f\"   Biometric: {self.bio_cols}\")\n",
    "    \n",
    "    def create_totals(self):\n",
    "        \"\"\"Create total columns for each category.\"\"\"\n",
    "        if self.enrol_cols:\n",
    "            numeric_enrol = [c for c in self.enrol_cols if self.df[c].dtype in ['int64', 'float64']]\n",
    "            if numeric_enrol:\n",
    "                self.df['total_enrolments'] = self.df[numeric_enrol].sum(axis=1)\n",
    "                self.features_created.append('total_enrolments')\n",
    "        \n",
    "        if self.demo_cols:\n",
    "            numeric_demo = [c for c in self.demo_cols if self.df[c].dtype in ['int64', 'float64']]\n",
    "            if numeric_demo:\n",
    "                self.df['total_demo_updates'] = self.df[numeric_demo].sum(axis=1)\n",
    "                self.features_created.append('total_demo_updates')\n",
    "        \n",
    "        if self.bio_cols:\n",
    "            numeric_bio = [c for c in self.bio_cols if self.df[c].dtype in ['int64', 'float64']]\n",
    "            if numeric_bio:\n",
    "                self.df['total_bio_updates'] = self.df[numeric_bio].sum(axis=1)\n",
    "                self.features_created.append('total_bio_updates')\n",
    "        \n",
    "        update_cols = ['total_demo_updates', 'total_bio_updates']\n",
    "        available_update_cols = [c for c in update_cols if c in self.df.columns]\n",
    "        if available_update_cols:\n",
    "            self.df['total_updates'] = self.df[available_update_cols].sum(axis=1)\n",
    "            self.features_created.append('total_updates')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_update_stress_ratio(self):\n",
    "        \"\"\"\n",
    "        Update Stress Ratio (USR)\n",
    "        = (Demographic Updates + Biometric Updates) / Enrolments\n",
    "        \n",
    "        High USR indicates infrastructure under update pressure.\n",
    "        \"\"\"\n",
    "        if 'total_updates' in self.df.columns and 'total_enrolments' in self.df.columns:\n",
    "            self.df['update_stress_ratio'] = np.where(\n",
    "                self.df['total_enrolments'] > 0,\n",
    "                self.df['total_updates'] / self.df['total_enrolments'],\n",
    "                0\n",
    "            )\n",
    "            self.features_created.append('update_stress_ratio')\n",
    "            print(\"   Created: Update Stress Ratio (USR)\")\n",
    "        return self\n",
    "    \n",
    "    def create_age_transition_index(self):\n",
    "        \"\"\"\n",
    "        Age Transition Index (ATI)\n",
    "        = bio_updates_18+ / demo_updates_5_17\n",
    "        \n",
    "        High ATI indicates child-to-adult transition pressure.\n",
    "        \"\"\"\n",
    "        bio_adult = [c for c in self.df.columns if 'bio' in c.lower() and ('17_' in c or '18' in c)]\n",
    "        demo_child = [c for c in self.df.columns if 'demo' in c.lower() and '5_17' in c]\n",
    "        \n",
    "        if bio_adult and demo_child:\n",
    "            self.df['age_transition_index'] = np.where(\n",
    "                self.df[demo_child[0]] > 0,\n",
    "                self.df[bio_adult[0]] / self.df[demo_child[0]],\n",
    "                0\n",
    "            )\n",
    "            self.features_created.append('age_transition_index')\n",
    "            print(\"   Created: Age Transition Index (ATI)\")\n",
    "        return self\n",
    "    \n",
    "    def create_temporal_features(self):\n",
    "        \"\"\"Extract temporal features from date column.\"\"\"\n",
    "        if 'date' in self.df.columns:\n",
    "            self.df['year'] = self.df['date'].dt.year\n",
    "            self.df['month'] = self.df['date'].dt.month\n",
    "            self.df['week'] = self.df['date'].dt.isocalendar().week\n",
    "            self.df['day_of_week'] = self.df['date'].dt.dayofweek\n",
    "            self.df['quarter'] = self.df['date'].dt.quarter\n",
    "            self.df['is_weekend'] = (self.df['day_of_week'] >= 5).astype(int)\n",
    "            self.df['is_month_start'] = self.df['date'].dt.is_month_start.astype(int)\n",
    "            self.df['is_month_end'] = self.df['date'].dt.is_month_end.astype(int)\n",
    "            \n",
    "            self.features_created.extend(['year', 'month', 'week', 'day_of_week', 'quarter', \n",
    "                                          'is_weekend', 'is_month_start', 'is_month_end'])\n",
    "            print(\"   Created: Temporal features\")\n",
    "        return self\n",
    "    \n",
    "    def create_rolling_features(self, window_sizes=[7, 14, 30]):\n",
    "        \"\"\"Create rolling statistics for trend analysis.\"\"\"\n",
    "        if 'date' not in self.df.columns:\n",
    "            return self\n",
    "        \n",
    "        self.df = self.df.sort_values('date')\n",
    "        \n",
    "        target_cols = ['total_updates', 'total_enrolments', 'update_stress_ratio']\n",
    "        available_targets = [c for c in target_cols if c in self.df.columns]\n",
    "        \n",
    "        group_cols = []\n",
    "        if 'state' in self.df.columns:\n",
    "            group_cols.append('state')\n",
    "        if 'district' in self.df.columns:\n",
    "            group_cols.append('district')\n",
    "        \n",
    "        for col in available_targets:\n",
    "            for window in window_sizes:\n",
    "                if group_cols:\n",
    "                    self.df[f'{col}_rolling_{window}d_mean'] = (\n",
    "                        self.df.groupby(group_cols)[col]\n",
    "                        .transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "                    )\n",
    "                    self.df[f'{col}_rolling_{window}d_std'] = (\n",
    "                        self.df.groupby(group_cols)[col]\n",
    "                        .transform(lambda x: x.rolling(window, min_periods=1).std())\n",
    "                    )\n",
    "                else:\n",
    "                    self.df[f'{col}_rolling_{window}d_mean'] = self.df[col].rolling(window, min_periods=1).mean()\n",
    "                    self.df[f'{col}_rolling_{window}d_std'] = self.df[col].rolling(window, min_periods=1).std()\n",
    "                \n",
    "                self.features_created.extend([f'{col}_rolling_{window}d_mean', f'{col}_rolling_{window}d_std'])\n",
    "        \n",
    "        print(f\"   Created: Rolling features for windows {window_sizes}\")\n",
    "        return self\n",
    "    \n",
    "    def create_volatility_score(self):\n",
    "        \"\"\"\n",
    "        Volatility Score = Coefficient of variation of updates.\n",
    "        High volatility indicates unstable infrastructure.\n",
    "        \"\"\"\n",
    "        if 'total_updates_rolling_7d_std' in self.df.columns and 'total_updates_rolling_7d_mean' in self.df.columns:\n",
    "            self.df['volatility_score'] = np.where(\n",
    "                self.df['total_updates_rolling_7d_mean'] > 0,\n",
    "                self.df['total_updates_rolling_7d_std'] / self.df['total_updates_rolling_7d_mean'],\n",
    "                0\n",
    "            )\n",
    "            self.features_created.append('volatility_score')\n",
    "            print(\"   Created: Volatility Score\")\n",
    "        return self\n",
    "    \n",
    "    def create_stress_labels(self, usr_thresholds=(3, 7)):\n",
    "        \"\"\"\n",
    "        Create stress classification labels based on USR.\n",
    "        \n",
    "        Stable: USR < lower_threshold\n",
    "        Watch: lower_threshold <= USR < upper_threshold\n",
    "        Critical: USR >= upper_threshold\n",
    "        \"\"\"\n",
    "        if 'update_stress_ratio' in self.df.columns:\n",
    "            lower, upper = usr_thresholds\n",
    "            conditions = [\n",
    "                self.df['update_stress_ratio'] < lower,\n",
    "                (self.df['update_stress_ratio'] >= lower) & (self.df['update_stress_ratio'] < upper),\n",
    "                self.df['update_stress_ratio'] >= upper\n",
    "            ]\n",
    "            labels = ['Stable', 'Watch', 'Critical']\n",
    "            self.df['stress_level'] = np.select(conditions, labels, default='Unknown')\n",
    "            \n",
    "            self.df['stress_level_encoded'] = self.df['stress_level'].map(\n",
    "                {'Stable': 0, 'Watch': 1, 'Critical': 2, 'Unknown': -1}\n",
    "            )\n",
    "            \n",
    "            self.features_created.extend(['stress_level', 'stress_level_encoded'])\n",
    "            print(f\"   Created: Stress labels (thresholds: {usr_thresholds})\")\n",
    "        return self\n",
    "    \n",
    "    def engineer(self):\n",
    "        \"\"\"Execute full feature engineering pipeline.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"FEATURE ENGINEERING\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        self.identify_columns()\n",
    "        print(\"\\nCreating features:\")\n",
    "        \n",
    "        self.create_totals()\n",
    "        self.create_update_stress_ratio()\n",
    "        self.create_age_transition_index()\n",
    "        self.create_temporal_features()\n",
    "        self.create_rolling_features()\n",
    "        self.create_volatility_score()\n",
    "        self.create_stress_labels()\n",
    "        \n",
    "        self.df = self.df.fillna(0)\n",
    "        \n",
    "        print(f\"\\nTotal features created: {len(self.features_created)}\")\n",
    "        print(f\"Final dataset shape: {self.df.shape}\")\n",
    "        \n",
    "        return self.df\n",
    "\n",
    "# Execute feature engineering\n",
    "if df_master is not None:\n",
    "    fe = FeatureEngineer(df_master)\n",
    "    df_featured = fe.engineer()\n",
    "else:\n",
    "    print(\"No data available for feature engineering.\")\n",
    "    df_featured = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "if df_featured is not None:\n",
    "    print(\"=\"*70)\n",
    "    print(\"FEATURE SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    key_features = ['update_stress_ratio', 'age_transition_index', 'volatility_score', 'stress_level']\n",
    "    available_features = [f for f in key_features if f in df_featured.columns]\n",
    "    \n",
    "    if available_features:\n",
    "        print(\"\\nKey Feature Statistics:\")\n",
    "        display(df_featured[available_features].describe())\n",
    "        \n",
    "        if 'stress_level' in df_featured.columns:\n",
    "            print(\"\\nStress Level Distribution:\")\n",
    "            print(df_featured['stress_level'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Comprehensive analysis including univariate, bivariate, and trivariate exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EDA FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "class ExploratoryAnalysis:\n",
    "    \"\"\"Comprehensive EDA for Aadhaar datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.insights = []\n",
    "    \n",
    "    def add_insight(self, category, finding):\n",
    "        self.insights.append({'category': category, 'finding': finding})\n",
    "    \n",
    "    def univariate_numeric(self, column, figsize=(12, 4)):\n",
    "        \"\"\"Univariate analysis for numeric columns.\"\"\"\n",
    "        if column not in self.df.columns:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "        \n",
    "        axes[0].hist(self.df[column].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
    "        axes[0].set_title(f'Distribution: {column}')\n",
    "        axes[0].set_xlabel(column)\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        \n",
    "        axes[1].boxplot(self.df[column].dropna())\n",
    "        axes[1].set_title(f'Box Plot: {column}')\n",
    "        axes[1].set_ylabel(column)\n",
    "        \n",
    "        stats_text = f\"Mean: {self.df[column].mean():.2f}\\n\"\n",
    "        stats_text += f\"Median: {self.df[column].median():.2f}\\n\"\n",
    "        stats_text += f\"Std: {self.df[column].std():.2f}\\n\"\n",
    "        stats_text += f\"Skew: {self.df[column].skew():.2f}\\n\"\n",
    "        stats_text += f\"Kurtosis: {self.df[column].kurtosis():.2f}\"\n",
    "        axes[2].text(0.1, 0.5, stats_text, transform=axes[2].transAxes, \n",
    "                     fontsize=12, verticalalignment='center', fontfamily='monospace')\n",
    "        axes[2].set_title(f'Statistics: {column}')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        skewness = self.df[column].skew()\n",
    "        if abs(skewness) > 1:\n",
    "            self.add_insight('Univariate', f'{column} is highly skewed ({skewness:.2f})')\n",
    "    \n",
    "    def univariate_categorical(self, column, top_n=15, figsize=(12, 5)):\n",
    "        \"\"\"Univariate analysis for categorical columns.\"\"\"\n",
    "        if column not in self.df.columns:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        value_counts = self.df[column].value_counts().head(top_n)\n",
    "        \n",
    "        value_counts.plot(kind='barh', ax=axes[0], color='steelblue', edgecolor='black')\n",
    "        axes[0].set_title(f'Top {top_n} Categories: {column}')\n",
    "        axes[0].set_xlabel('Count')\n",
    "        axes[0].invert_yaxis()\n",
    "        \n",
    "        top_10 = value_counts.head(10)\n",
    "        axes[1].pie(top_10.values, labels=top_10.index, autopct='%1.1f%%', startangle=90)\n",
    "        axes[1].set_title(f'Proportion (Top 10): {column}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        top_category = value_counts.index[0]\n",
    "        top_pct = (value_counts.iloc[0] / len(self.df) * 100)\n",
    "        self.add_insight('Univariate', f'{column}: \"{top_category}\" dominates with {top_pct:.1f}%')\n",
    "    \n",
    "    def bivariate_scatter(self, x_col, y_col, hue_col=None, figsize=(10, 6)):\n",
    "        \"\"\"Scatter plot with optional grouping.\"\"\"\n",
    "        if x_col not in self.df.columns or y_col not in self.df.columns:\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        if hue_col and hue_col in self.df.columns:\n",
    "            for group in self.df[hue_col].unique():\n",
    "                subset = self.df[self.df[hue_col] == group]\n",
    "                ax.scatter(subset[x_col], subset[y_col], label=group, alpha=0.6)\n",
    "            ax.legend(title=hue_col)\n",
    "        else:\n",
    "            ax.scatter(self.df[x_col], self.df[y_col], alpha=0.6)\n",
    "        \n",
    "        ax.set_xlabel(x_col)\n",
    "        ax.set_ylabel(y_col)\n",
    "        ax.set_title(f'{y_col} vs {x_col}')\n",
    "        \n",
    "        valid_data = self.df[[x_col, y_col]].dropna()\n",
    "        if len(valid_data) > 2:\n",
    "            corr, _ = pearsonr(valid_data[x_col], valid_data[y_col])\n",
    "            ax.annotate(f'Correlation: {corr:.3f}', xy=(0.05, 0.95), xycoords='axes fraction')\n",
    "            self.add_insight('Bivariate', f'{x_col} vs {y_col}: r = {corr:.3f}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def correlation_matrix(self, columns=None, figsize=(12, 10)):\n",
    "        \"\"\"Correlation heatmap for numeric columns.\"\"\"\n",
    "        if columns is None:\n",
    "            columns = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        columns = columns[:20]\n",
    "        corr_matrix = self.df[columns].corr()\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "                    center=0, square=True, linewidths=0.5, ax=ax, cbar_kws={'shrink': 0.8})\n",
    "        ax.set_title('Feature Correlation Matrix')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def trivariate_heatmap(self, x_col, y_col, value_col, aggfunc='mean', figsize=(14, 10)):\n",
    "        \"\"\"Pivot table heatmap for three variables.\"\"\"\n",
    "        if not all(c in self.df.columns for c in [x_col, y_col, value_col]):\n",
    "            return\n",
    "        \n",
    "        pivot = self.df.pivot_table(values=value_col, index=y_col, columns=x_col, aggfunc=aggfunc)\n",
    "        pivot = pivot.iloc[:20, :20]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        sns.heatmap(pivot, annot=True, fmt='.2f', cmap='YlOrRd', ax=ax)\n",
    "        ax.set_title(f'{aggfunc.title()} {value_col} by {x_col} and {y_col}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def temporal_trend(self, date_col, value_col, group_col=None, figsize=(14, 6)):\n",
    "        \"\"\"Time series trend analysis.\"\"\"\n",
    "        if date_col not in self.df.columns or value_col not in self.df.columns:\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        if group_col and group_col in self.df.columns:\n",
    "            top_groups = self.df[group_col].value_counts().head(5).index\n",
    "            for group in top_groups:\n",
    "                subset = self.df[self.df[group_col] == group].groupby(date_col)[value_col].mean()\n",
    "                ax.plot(subset.index, subset.values, label=group, marker='o', markersize=3)\n",
    "            ax.legend(title=group_col)\n",
    "        else:\n",
    "            trend_data = self.df.groupby(date_col)[value_col].mean()\n",
    "            ax.plot(trend_data.index, trend_data.values, marker='o', markersize=3)\n",
    "        \n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel(value_col)\n",
    "        ax.set_title(f'Temporal Trend: {value_col}')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def geographic_summary(self, geo_col='state', value_col='update_stress_ratio', figsize=(14, 8)):\n",
    "        \"\"\"Geographic distribution analysis.\"\"\"\n",
    "        if geo_col not in self.df.columns or value_col not in self.df.columns:\n",
    "            return None\n",
    "        \n",
    "        geo_agg = self.df.groupby(geo_col).agg({\n",
    "            value_col: ['mean', 'std', 'count']\n",
    "        }).round(3)\n",
    "        geo_agg.columns = ['mean', 'std', 'count']\n",
    "        geo_agg = geo_agg.sort_values('mean', ascending=False).head(20)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        geo_agg['mean'].plot(kind='barh', ax=axes[0], color='steelblue', edgecolor='black')\n",
    "        axes[0].set_title(f'Mean {value_col} by {geo_col}')\n",
    "        axes[0].set_xlabel(f'Mean {value_col}')\n",
    "        axes[0].invert_yaxis()\n",
    "        \n",
    "        axes[1].barh(geo_agg.index, geo_agg['mean'], xerr=geo_agg['std'], color='coral', \n",
    "                     edgecolor='black', capsize=3)\n",
    "        axes[1].set_title(f'{value_col} with Std Dev by {geo_col}')\n",
    "        axes[1].set_xlabel(value_col)\n",
    "        axes[1].invert_yaxis()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        top_stress = geo_agg.head(3).index.tolist()\n",
    "        self.add_insight('Geographic', f'Highest stress in: {\", \".join(top_stress)}')\n",
    "        \n",
    "        return geo_agg\n",
    "    \n",
    "    def get_insights(self):\n",
    "        return pd.DataFrame(self.insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXECUTE EXPLORATORY ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "if df_featured is not None:\n",
    "    eda = ExploratoryAnalysis(df_featured)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"UNIVARIATE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if 'update_stress_ratio' in df_featured.columns:\n",
    "        eda.univariate_numeric('update_stress_ratio')\n",
    "    \n",
    "    if 'total_updates' in df_featured.columns:\n",
    "        eda.univariate_numeric('total_updates')\n",
    "    \n",
    "    if 'state' in df_featured.columns:\n",
    "        eda.univariate_categorical('state')\n",
    "    \n",
    "    if 'stress_level' in df_featured.columns:\n",
    "        eda.univariate_categorical('stress_level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BIVARIATE AND TRIVARIATE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "if df_featured is not None:\n",
    "    print(\"=\"*70)\n",
    "    print(\"BIVARIATE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if 'total_enrolments' in df_featured.columns and 'total_updates' in df_featured.columns:\n",
    "        eda.bivariate_scatter('total_enrolments', 'total_updates', \n",
    "                             hue_col='stress_level' if 'stress_level' in df_featured.columns else None)\n",
    "    \n",
    "    key_numeric = ['total_enrolments', 'total_updates', 'total_demo_updates', 'total_bio_updates',\n",
    "                   'update_stress_ratio', 'age_transition_index', 'volatility_score']\n",
    "    available_numeric = [c for c in key_numeric if c in df_featured.columns]\n",
    "    if len(available_numeric) >= 3:\n",
    "        eda.correlation_matrix(available_numeric)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"TRIVARIATE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if all(c in df_featured.columns for c in ['state', 'month', 'update_stress_ratio']):\n",
    "        eda.trivariate_heatmap('month', 'state', 'update_stress_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEMPORAL AND GEOGRAPHIC ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "if df_featured is not None:\n",
    "    print(\"=\"*70)\n",
    "    print(\"TEMPORAL AND GEOGRAPHIC ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if 'date' in df_featured.columns and 'update_stress_ratio' in df_featured.columns:\n",
    "        eda.temporal_trend('date', 'update_stress_ratio', \n",
    "                          group_col='state' if 'state' in df_featured.columns else None)\n",
    "    \n",
    "    if 'state' in df_featured.columns and 'update_stress_ratio' in df_featured.columns:\n",
    "        state_summary = eda.geographic_summary('state', 'update_stress_ratio')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"KEY INSIGHTS FROM EDA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    insights_df = eda.get_insights()\n",
    "    if not insights_df.empty:\n",
    "        for idx, row in insights_df.iterrows():\n",
    "            print(f\"[{row['category']}] {row['finding']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predictive Modeling Layer\n",
    "\n",
    "Three complementary models:\n",
    "1. Stress Classification Model (Gradient Boosting)\n",
    "2. Time-Series Forecasting\n",
    "3. Anomaly Detection (Isolation Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL 1: STRESS CLASSIFICATION\n",
    "# =============================================================================\n",
    "\n",
    "class StressClassifier:\n",
    "    \"\"\"\n",
    "    Classifies each district/pincode/time period as:\n",
    "    - Stable (low stress)\n",
    "    - Watch (moderate stress)  \n",
    "    - Critical (high stress)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_importance = None\n",
    "        self.feature_cols = []\n",
    "    \n",
    "    def prepare_features(self):\n",
    "        \"\"\"Prepare feature matrix and target variable.\"\"\"\n",
    "        potential_features = [\n",
    "            'total_enrolments', 'total_updates', 'total_demo_updates', 'total_bio_updates',\n",
    "            'age_transition_index', 'volatility_score',\n",
    "            'total_updates_rolling_7d_mean', 'total_updates_rolling_7d_std',\n",
    "            'total_updates_rolling_14d_mean', 'total_updates_rolling_30d_mean',\n",
    "            'month', 'quarter', 'day_of_week', 'is_weekend'\n",
    "        ]\n",
    "        \n",
    "        self.feature_cols = [c for c in potential_features if c in self.df.columns]\n",
    "        \n",
    "        if 'stress_level_encoded' not in self.df.columns:\n",
    "            print(\"Error: stress_level_encoded not found.\")\n",
    "            return None, None\n",
    "        \n",
    "        X = self.df[self.feature_cols].copy()\n",
    "        y = self.df['stress_level_encoded'].copy()\n",
    "        \n",
    "        valid_mask = y >= 0\n",
    "        X = X[valid_mask]\n",
    "        y = y[valid_mask]\n",
    "        X = X.fillna(0)\n",
    "        \n",
    "        print(f\"Features selected: {len(self.feature_cols)}\")\n",
    "        print(f\"Training samples: {len(X):,}\")\n",
    "        print(f\"Class distribution:\\n{y.value_counts().sort_index()}\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def train(self, test_size=0.2, random_state=42):\n",
    "        \"\"\"Train the stress classification model.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"MODEL 1: STRESS CLASSIFICATION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        X, y = self.prepare_features()\n",
    "        if X is None:\n",
    "            return self\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        print(\"\\nTraining Gradient Boosting Classifier...\")\n",
    "        self.model = GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        self.model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        y_pred = self.model.predict(X_test_scaled)\n",
    "        \n",
    "        print(\"\\n--- Classification Report ---\")\n",
    "        target_names = ['Stable', 'Watch', 'Critical']\n",
    "        print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "        \n",
    "        cv_scores = cross_val_score(self.model, X_train_scaled, y_train, cv=5, scoring='f1_weighted')\n",
    "        print(f\"Cross-validation F1: {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})\")\n",
    "        \n",
    "        self.feature_importance = pd.DataFrame({\n",
    "            'feature': self.feature_cols,\n",
    "            'importance': self.model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        self.y_test = y_test\n",
    "        self.y_pred = y_pred\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"Visualize model results.\"\"\"\n",
    "        if self.model is None:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        cm = confusion_matrix(self.y_test, self.y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "                    xticklabels=['Stable', 'Watch', 'Critical'],\n",
    "                    yticklabels=['Stable', 'Watch', 'Critical'])\n",
    "        axes[0].set_title('Confusion Matrix')\n",
    "        axes[0].set_xlabel('Predicted')\n",
    "        axes[0].set_ylabel('Actual')\n",
    "        \n",
    "        top_features = self.feature_importance.head(10)\n",
    "        axes[1].barh(top_features['feature'], top_features['importance'], \n",
    "                     color='steelblue', edgecolor='black')\n",
    "        axes[1].set_title('Top 10 Feature Importance')\n",
    "        axes[1].set_xlabel('Importance')\n",
    "        axes[1].invert_yaxis()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Train stress classifier\n",
    "if df_featured is not None and 'stress_level_encoded' in df_featured.columns:\n",
    "    stress_classifier = StressClassifier(df_featured)\n",
    "    stress_classifier.train()\n",
    "    stress_classifier.plot_results()\n",
    "else:\n",
    "    print(\"Stress classification requires engineered features. Skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL 2: TIME-SERIES FORECASTING\n",
    "# =============================================================================\n",
    "\n",
    "class UpdateForecaster:\n",
    "    \"\"\"Forecasts future update load using time-series methods.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, target_col='total_updates', date_col='date'):\n",
    "        self.df = df\n",
    "        self.target_col = target_col\n",
    "        self.date_col = date_col\n",
    "    \n",
    "    def prepare_series(self):\n",
    "        \"\"\"Prepare time series data.\"\"\"\n",
    "        if self.date_col not in self.df.columns or self.target_col not in self.df.columns:\n",
    "            return None\n",
    "        \n",
    "        ts_data = self.df.groupby(self.date_col)[self.target_col].sum().reset_index()\n",
    "        ts_data = ts_data.set_index(self.date_col).sort_index()\n",
    "        return ts_data\n",
    "    \n",
    "    def decompose(self, ts_data, period=7):\n",
    "        \"\"\"Decompose time series.\"\"\"\n",
    "        if ts_data is None or len(ts_data) < period * 2:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            decomposition = seasonal_decompose(ts_data[self.target_col], model='additive', period=period)\n",
    "            \n",
    "            fig, axes = plt.subplots(4, 1, figsize=(14, 10))\n",
    "            decomposition.observed.plot(ax=axes[0], title='Observed')\n",
    "            decomposition.trend.plot(ax=axes[1], title='Trend')\n",
    "            decomposition.seasonal.plot(ax=axes[2], title='Seasonal')\n",
    "            decomposition.resid.plot(ax=axes[3], title='Residual')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            return decomposition\n",
    "        except Exception as e:\n",
    "            print(f\"Decomposition error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def forecast(self, ts_data, horizon=30):\n",
    "        \"\"\"Simple forecast using trend extrapolation.\"\"\"\n",
    "        if ts_data is None or len(ts_data) < 14:\n",
    "            return None\n",
    "        \n",
    "        recent_mean = ts_data[self.target_col].tail(14).mean()\n",
    "        recent_std = ts_data[self.target_col].tail(14).std()\n",
    "        trend = (ts_data[self.target_col].tail(7).mean() - \n",
    "                 ts_data[self.target_col].tail(14).head(7).mean()) / 7\n",
    "        \n",
    "        last_date = ts_data.index[-1]\n",
    "        forecast_dates = pd.date_range(start=last_date + timedelta(days=1), periods=horizon)\n",
    "        forecast_values = [recent_mean + trend * i for i in range(1, horizon + 1)]\n",
    "        \n",
    "        forecast_df = pd.DataFrame({\n",
    "            'date': forecast_dates,\n",
    "            'forecast': forecast_values,\n",
    "            'lower_bound': [v - 1.96 * recent_std for v in forecast_values],\n",
    "            'upper_bound': [v + 1.96 * recent_std for v in forecast_values]\n",
    "        })\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "        ax.plot(ts_data.index, ts_data[self.target_col], label='Historical', color='blue')\n",
    "        ax.plot(forecast_df['date'], forecast_df['forecast'], label='Forecast', \n",
    "                color='red', linestyle='--')\n",
    "        ax.fill_between(forecast_df['date'], forecast_df['lower_bound'], \n",
    "                       forecast_df['upper_bound'], color='red', alpha=0.2, label='95% CI')\n",
    "        ax.set_title(f'{horizon}-Day Forecast: {self.target_col}')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel(self.target_col)\n",
    "        ax.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return forecast_df\n",
    "\n",
    "# Run forecasting\n",
    "if df_featured is not None and 'date' in df_featured.columns:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL 2: TIME-SERIES FORECASTING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    target = 'total_updates' if 'total_updates' in df_featured.columns else 'total_enrolments'\n",
    "    if target in df_featured.columns:\n",
    "        forecaster = UpdateForecaster(df_featured, target_col=target)\n",
    "        ts_data = forecaster.prepare_series()\n",
    "        if ts_data is not None and len(ts_data) > 14:\n",
    "            forecaster.decompose(ts_data)\n",
    "            forecast = forecaster.forecast(ts_data, horizon=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL 3: ANOMALY DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "class AnomalyDetector:\n",
    "    \"\"\"Detects unusual patterns using Isolation Forest.\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.model = None\n",
    "        self.anomalies = None\n",
    "        self.feature_cols = []\n",
    "    \n",
    "    def detect(self, contamination=0.05):\n",
    "        \"\"\"Run anomaly detection.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"MODEL 3: ANOMALY DETECTION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        potential_features = [\n",
    "            'total_updates', 'total_enrolments', 'update_stress_ratio',\n",
    "            'age_transition_index', 'volatility_score',\n",
    "            'total_updates_rolling_7d_mean', 'total_updates_rolling_7d_std'\n",
    "        ]\n",
    "        \n",
    "        self.feature_cols = [c for c in potential_features if c in self.df.columns]\n",
    "        \n",
    "        if not self.feature_cols:\n",
    "            print(\"No suitable features found.\")\n",
    "            return self\n",
    "        \n",
    "        X = self.df[self.feature_cols].fillna(0)\n",
    "        print(f\"Features: {self.feature_cols}\")\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        print(f\"Training Isolation Forest (contamination={contamination})...\")\n",
    "        self.model = IsolationForest(\n",
    "            contamination=contamination,\n",
    "            random_state=42,\n",
    "            n_estimators=100\n",
    "        )\n",
    "        \n",
    "        predictions = self.model.fit_predict(X_scaled)\n",
    "        scores = self.model.decision_function(X_scaled)\n",
    "        \n",
    "        self.df['anomaly'] = predictions\n",
    "        self.df['anomaly_score'] = scores\n",
    "        self.df['is_anomaly'] = (predictions == -1).astype(int)\n",
    "        \n",
    "        self.anomalies = self.df[self.df['is_anomaly'] == 1].copy()\n",
    "        \n",
    "        print(f\"\\nTotal records: {len(self.df):,}\")\n",
    "        print(f\"Anomalies detected: {len(self.anomalies):,} ({len(self.anomalies)/len(self.df)*100:.2f}%)\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def analyze(self):\n",
    "        \"\"\"Analyze detected anomalies.\"\"\"\n",
    "        if self.anomalies is None or len(self.anomalies) == 0:\n",
    "            return\n",
    "        \n",
    "        print(\"\\n--- Anomaly Analysis ---\")\n",
    "        \n",
    "        if 'state' in self.anomalies.columns:\n",
    "            print(\"\\nTop States with Anomalies:\")\n",
    "            print(self.anomalies['state'].value_counts().head(10))\n",
    "        \n",
    "        if 'district' in self.anomalies.columns:\n",
    "            print(\"\\nTop Districts with Anomalies:\")\n",
    "            print(self.anomalies['district'].value_counts().head(10))\n",
    "        \n",
    "        print(\"\\n--- Anomaly vs Normal Comparison ---\")\n",
    "        normal = self.df[self.df['is_anomaly'] == 0]\n",
    "        \n",
    "        comparison = pd.DataFrame({\n",
    "            'Normal_Mean': normal[self.feature_cols].mean(),\n",
    "            'Anomaly_Mean': self.anomalies[self.feature_cols].mean(),\n",
    "        })\n",
    "        comparison['Difference_%'] = ((comparison['Anomaly_Mean'] - comparison['Normal_Mean']) / \n",
    "                                       comparison['Normal_Mean'] * 100).round(2)\n",
    "        print(comparison)\n",
    "    \n",
    "    def plot(self):\n",
    "        \"\"\"Visualize anomalies.\"\"\"\n",
    "        if self.anomalies is None:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        axes[0, 0].hist(self.df['anomaly_score'], bins=50, edgecolor='black', alpha=0.7)\n",
    "        axes[0, 0].set_title('Anomaly Score Distribution')\n",
    "        axes[0, 0].set_xlabel('Anomaly Score')\n",
    "        \n",
    "        if 'total_updates' in self.df.columns and 'update_stress_ratio' in self.df.columns:\n",
    "            normal = self.df[self.df['is_anomaly'] == 0]\n",
    "            anomaly = self.df[self.df['is_anomaly'] == 1]\n",
    "            axes[0, 1].scatter(normal['total_updates'], normal['update_stress_ratio'], \n",
    "                              alpha=0.3, label='Normal', s=10)\n",
    "            axes[0, 1].scatter(anomaly['total_updates'], anomaly['update_stress_ratio'], \n",
    "                              color='red', alpha=0.8, label='Anomaly', s=30, marker='x')\n",
    "            axes[0, 1].set_title('Anomalies in Update Space')\n",
    "            axes[0, 1].legend()\n",
    "        \n",
    "        if 'state' in self.anomalies.columns:\n",
    "            state_anomalies = self.anomalies['state'].value_counts().head(10)\n",
    "            axes[1, 0].barh(state_anomalies.index, state_anomalies.values, \n",
    "                           color='coral', edgecolor='black')\n",
    "            axes[1, 0].set_title('Top 10 States by Anomaly Count')\n",
    "            axes[1, 0].invert_yaxis()\n",
    "        \n",
    "        if 'date' in self.df.columns:\n",
    "            anomaly_trend = self.df.groupby('date')['is_anomaly'].sum()\n",
    "            axes[1, 1].plot(anomaly_trend.index, anomaly_trend.values, color='red', marker='o', markersize=3)\n",
    "            axes[1, 1].set_title('Anomalies Over Time')\n",
    "            plt.setp(axes[1, 1].xaxis.get_majorticklabels(), rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_flagged_locations(self, top_n=20):\n",
    "        \"\"\"Get top flagged locations.\"\"\"\n",
    "        if self.anomalies is None:\n",
    "            return None\n",
    "        \n",
    "        group_cols = [c for c in


You're out of extra usage
Buy more
Continue


Claude is AI and can make mistakes. Please double-check responses.
Apix aadhaar hackathon Â· IPYNB
Copy

No file content available


Notification Youâ€™ve hit your limit for Claude messages. Limits will reset at 1:30 AM.  View your usage details.